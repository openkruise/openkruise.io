"use strict";(self.webpackChunkopenkruise_io=self.webpackChunkopenkruise_io||[]).push([[1477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"openkruise-1.3","metadata":{"permalink":"/blog/openkruise-1.3","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2022-10-07-release-1.3.md","source":"@site/blog/2022-10-07-release-1.3.md","title":"OpenKruise v1.3, New Custom Pod Probe Capabilities and Significant Performance Improvements for Large-Scale Clusters","description":"We\u2019re pleased to announce the release of OpenKruise 1.3, which is a CNCF Sandbox level project.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":8.495,"truncated":false,"authors":[{"name":"Mingshan Zhao","title":"Member of OpenKruise","url":"https://github.com/zmberg","imageURL":"https://github.com/zmberg.png","key":"zmberg"}],"frontMatter":{"slug":"openkruise-1.3","title":"OpenKruise v1.3, New Custom Pod Probe Capabilities and Significant Performance Improvements for Large-Scale Clusters","authors":["zmberg"],"tags":["release"]},"nextItem":{"title":"OpenKruise v1.2, new PersistentPodState feature to achieve IP retention","permalink":"/blog/openkruise-1.2"}},"content":"We\u2019re pleased to announce the release of OpenKruise 1.3, which is a CNCF Sandbox level project.\\n\\n[OpenKruise](https://openkruise.io) is an extended component suite for Kubernetes, which mainly focuses on application automations, such as deployment, upgrade, ops and availability protection. Mostly features provided by OpenKruise are built primarily based on CRD extensions. They can work in pure Kubernetes clusters without any other dependences.\\n\\n## What\'s new?\\n\\nIn release v1.3, OpenKruise provides a new CRD named `PodProbeMarker`, improves its performance in large-scale clusters, Advanced DaemonSet support pre-download image,\\nand some new features have been added to CloneSet, WorkloadSpread, AdvancedCronJob, SidecarSet etc.\\n\\nHere we are going to introduce some changes of it.\\n\\n### 1. New CRD and Controller: PodProbeMarker\\n\\nKubernetes provides three Pod lifecycle management:\\n- **Readiness Probe** Used to determine whether the business container is ready to respond to user requests. If the probe fails, the Pod will be removed from Service Endpoints.\\n- **Liveness Probe** Used to determine the health status of the container. If the probe fails, the kubelet will restart the container.\\n- **Startup Probe** Used to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds.\\n\\nSo the Probe capabilities provided in Kubernetes have defined specific semantics and related behaviors.\\n**In addition, there is actually a need to customize Probe semantics and related behaviors**, such as:\\n- **GameServer defines Idle Probe to determine whether the Pod currently has a game match**, if not, from the perspective of cost optimization, the Pod can be scaled down.\\n- **K8S Operator defines the main-secondary probe to determine the role of the current Pod (main or secondary)**. When upgrading, the secondary can be upgraded first,\\nso as to achieve the behavior of selecting the main only once during the upgrade process, reducing the service interruption time during the upgrade process.\\n\\nOpenKruise provides the ability to customize the Probe and return the result to the Pod Status, and the user can decide the follow-up behavior based on the probe result.\\n\\nAn object of PodProbeMarker may look like this:\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: PodProbeMarker\\nmetadata:\\n  name: game-server-probe\\n  namespace: ns\\nspec:\\n  selector:\\n    matchLabels:\\n    app: game-server\\n  probes:\\n  - name: Idle\\n    containerName: game-server\\n    probe:\\n      exec: /home/game/idle.sh\\n      initialDelaySeconds: 10\\n      timeoutSeconds: 3\\n      periodSeconds: 10\\n      successThreshold: 1\\n      failureThreshold: 3\\n    markerPolicy:\\n    - state: Succeeded\\n      labels:\\n        gameserver-idle: \'true\'\\n      annotations:\\n        controller.kubernetes.io/pod-deletion-cost: \'-10\'\\n    - state: Failed\\n      labels:\\n        gameserver-idle: \'false\'\\n      annotations:\\n        controller.kubernetes.io/pod-deletion-cost: \'10\'\\n    podConditionType: game.io/idle\\n```\\n\\nPodProbeMarker results can be viewed at Pod Object:\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    app: game-server\\n    gameserver-idle: \'true\'\\n  annotations:\\n    controller.kubernetes.io/pod-deletion-cost: \'-10\'\\n  name: game-server-58cb9f5688-7sbd8\\n  namespace: ns\\nspec:\\n  ...\\nstatus:\\n  conditions:\\n    # podConditionType\\n  - type: game.io/idle\\n    # Probe State \'Succeeded\' indicates \'True\', and \'Failed\' indicates \'False\'\\n    status: \\"True\\"\\n    lastProbeTime: \\"2022-09-09T07:13:04Z\\"\\n    lastTransitionTime: \\"2022-09-09T07:13:04Z\\"\\n    # If the probe fails to execute, the message is stderr\\n    message: \\"\\"\\n```\\n\\n### 2. Performance optimization: significant performance improvements for large-scale clusters\\n\\n- [#1026](https://github.com/openkruise/kruise/pull/1026) The introduction of a delayed queueing mechanism significantly optimizes the CloneSet controller work queue buildup problem when kruise-manager is pulled up in large-scale application clusters,\\nideally reducing initialization time by more than 80%.\\n- [#1027](https://github.com/openkruise/kruise/pull/1027) Optimize PodUnavailableBudget controller Event Handler logic to reduce the number of irrelevant Pods in the queue.\\n- [#1011](https://github.com/openkruise/kruise/pull/1011) The caching mechanism optimizes the CPU and Memory consumption of Advanced DaemonSet\'s repetitive simulation of Pod scheduling computations in large-scale clusters.\\n- [#1015](https://github.com/openkruise/kruise/pull/1015), [#1068](https://github.com/openkruise/kruise/pull/1068) Significantly reduce runtime memory consumption in large clusters. Complete the Disable DeepCopy feature started in v1.1, and reduce the conversion consumption of expressions type label selector.\\n\\n### 3. SidecarSet support inject specific historical version\\n\\nSidecarSet will record historical revision of some fields such as `containers`, `volumes`, `initContainers`, `imagePullSecrets` and `patchPodMetadata` via ControllerRevision.\\n**Based on this feature, you can easily select a specific historical revision to inject when creating Pods, rather than always inject the latest revision of sidecar.**\\n\\nSidecarSet records ControllerRevision in the same namespace as Kruse Manager. You can execute `kubectl get controllerrvisions -n kruise-system -l kruise.io/sidecarset-name=<your-sidecarset-name>` to list the ControllerRevisions of your SidecarSet.\\nMoreover, the ControllerRevision name of current SidecarSet revision is shown in `status.latestRevision` field, so you can record it very easily.\\n\\nThere are two configuration methods as follows:\\n\\n#### select revision via ControllerRevision name\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\nspec:\\n  ...\\n  updateStrategy:\\n    partition: 90%\\n  injectionStrategy:\\n    revisionName: <specific-controllerrevision-name>\\n```\\n\\n#### select revision via custom version label\\nYou can add or update the label `apps.kruise.io/sidecarset-custom-version=<your-version-id>` to SidecarSet when creating or publishing SidecarSet, to mark each historical revision.\\nSidecarSet will bring this label down to the corresponding ControllerRevision object, and you can easily use the `<your-version-id>` to describe which historical revision you want to inject.\\n\\nAssume that you are publishing `version-2` in canary way (you wish only 10% Pods will be upgraded), and you want to inject the stable `version-1` to newly-created Pods to reduce the risk of the canary publishing:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\n  labels:\\n    apps.kruise.io/sidecarset-custom-version: example/version-2\\nspec:\\n  ...\\n  updateStrategy:\\n    partition: 90%\\n  injectionStrategy:\\n    customVersion: example/version-1\\n```\\n\\n### 4. SidecarSet support inject pod annotations\\n\\nSidecarSet support inject pod annotations, as follows:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nspec:\\n  containers:\\n    ...\\n  patchPodMetadata:\\n  - annotations:\\n      oom-score: \'{\\"log-agent\\": 1}\'\\n      custom.example.com/sidecar-configuration: \'{\\"command\\": \\"/home/admin/bin/start.sh\\", \\"log-level\\": \\"3\\"}\'\\n    patchPolicy: MergePatchJson\\n  - annotations:\\n    apps.kruise.io/container-launch-priority: Ordered\\n  patchPolicy: Overwrite | Retain\\n```\\npatchPolicy is the injected policy, as follows:\\n- **Retain:** By default, if annotation[key]=value exists in the Pod, the original value of the Pod will be retained. Inject annotations[key]=value2 only if annotation[key] does not exist in the Pod.\\n- **Overwrite:** Corresponding to Retain, when annotation[key]=value exists in the Pod, it will be overwritten value2.\\n- **MergePatchJson:** Corresponding to Overwrite, the annotations value is a json string. If the annotations[key] does not exist in the Pod, it will be injected directly. If it exists, do a json value merge.\\nFor example: annotations[oom-score]=\'{\\"main\\": 2}\' exists in the Pod, after injection, the value json is merged into annotations[oom-score]=\'{\\"log-agent\\": 1, \\"main\\": 2}\'.\\n\\n**Note:** When the patchPolicy is Overwrite and MergePatchJson, the annotations can be updated synchronously when the SidecarSet in-place update the Sidecar Container.\\nHowever, if only the annotations are modified, it will not take effect. It must be in-place update together with the sidecar container image.\\nWhen patchPolicy is Retain, the annotations will not be updated when the SidecarSet in-place update the Sidecar Container.\\n\\nAccording to the above configuration, when the sidecarSet is injected into the sidecar container, it will inject Pod annotations synchronously, as follows:\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  annotations:\\n    apps.kruise.io/container-launch-priority: Ordered\\n    oom-score: \'{\\"log-agent\\": 1, \\"main\\": 2}\'\\n    custom.example.com/sidecar-configuration: \'{\\"command\\": \\"/home/admin/bin/start.sh\\", \\"log-level\\": \\"3\\"}\'\\nname: test-pod\\nspec:\\n  containers:\\n    ...\\n```\\n\\n**Note:** SidecarSet should not modify any configuration outside the sidecar container for security consideration, so if you want to use this capability, you need to first configure SidecarSet_PatchPodMetadata_WhiteList whitelist\\nor turn off whitelist checks via Feature-gate SidecarSetPatchPodMetadataDefaultsAllowed=true.\\n\\n### 5. Advanced DaemonSet support pre-downloading image for update\\n\\nIf you have enabled the `PreDownloadImageForDaemonSetUpdate` feature-gate,\\nDaemonSet controller will automatically pre-download the image you want to update to the nodes of all old Pods.\\nIt is quite useful to accelerate the progress of applications upgrade.\\n\\nThe parallelism of each new image pre-downloading by DaemonSet is `1`, which means the image is downloaded on nodes one by one.\\nYou can change the parallelism using `apps.kruise.io/image-predownload-parallelism` annotation on DaemonSet according to the capability of image registry,\\nfor registries with more bandwidth and P2P image downloading ability, a larger parallelism can speed up the pre-download process.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: DaemonSet\\nmetadata:\\n  annotations:\\n    apps.kruise.io/image-predownload-parallelism: \\"10\\"\\n```\\n\\n### 6. CloneSet Scaling with PreparingDelete\\n\\nCloneSet considers Pods in `PreparingDelete` state as normal by default, which means these Pods will still be calculated in the `replicas` number.\\n\\nIn this situation:\\n\\n- if you scale down `replicas` from `N` to `N-1`, when the Pod to be deleted is still in `PreparingDelete`, you scale up `replicas` to `N`, the CloneSet will move the Pod back to `Normal`.\\n- if you scale down `replicas` from `N` to `N-1` and put a Pod into `podsToDelete`, when the specific Pod is still in `PreparingDelete`, you scale up `replicas` to `N`, the CloneSet will not create a new Pod until the specific Pod goes into terminating.\\n- if you specificly delete a Pod without `replicas` changed, when the specific Pod is still in `PreparingDelete`, the CloneSet will not create a new Pod until the specific Pod goes into terminating.\\n\\nSince Kruise v1.3.0, you can put a `apps.kruise.io/cloneset-scaling-exclude-preparing-delete: \\"true\\"` label into CloneSet, which indicates Pods in `PreparingDelete` will not be calculated in the `replicas` number.\\n\\nIn this situation:\\n\\n- if you scale down `replicas` from `N` to `N-1`, when the Pod to be deleted is still in `PreparingDelete`, you scale up `replicas` to `N`, the CloneSet will move the Pod back to `Normal`.\\n- if you scale down `replicas` from `N` to `N-1` and put a Pod into `podsToDelete`, even if the specific Pod is still in `PreparingDelete`, you scale up `replicas` to `N`, the CloneSet will create a new Pod immediately.\\n- if you specificly delete a Pod without `replicas` changed, even if the specific Pod is still in `PreparingDelete`, the CloneSet will create a new Pod immediately.\\n\\n### 7. Advanced CronJob Time zones\\n\\nAll CronJob schedule: times are based on the timezone of the kruise-controller-manager by default,\\nwhich means the timezone set for the kruise-controller-manager container determines the timezone that the cron job controller uses.\\n\\nHowever, we have introduce a `spec.timeZone` field in v1.3.0.\\nYou can set it to the name of a valid time zone name. For example, setting `spec.timeZone: \\"Etc/UTC\\"` instructs Kruise to interpret the schedule relative to Coordinated Universal Time.\\n\\nA time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.\\n\\n### 8. Other changes\\n\\nFor more changes, their authors and commits, you can read the [Github release](https://github.com/openkruise/kruise/releases).\\n\\n## Get Involved\\n\\nWelcome to get involved with OpenKruise by joining us in Github/Slack/DingTalk/WeChat.\\nHave something you\u2019d like to broadcast to our community?\\nShare your voice at our [Bi-weekly community meeting (Chinese)](https://shimo.im/docs/gXqmeQOYBehZ4vqo), or through the channels below:\\n\\n- Join the community on [Slack](https://kubernetes.slack.com/channels/openkruise) (English).\\n- Join the community on DingTalk: Search GroupID `23330762` (Chinese).\\n- Join the community on WeChat (new): Search User `openkruise` and let the robot invite you (Chinese)."},{"id":"openkruise-1.2","metadata":{"permalink":"/blog/openkruise-1.2","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2022-06-07-release-1.2.md","source":"@site/blog/2022-06-07-release-1.2.md","title":"OpenKruise v1.2, new PersistentPodState feature to achieve IP retention","description":"We\u2019re pleased to announce the release of OpenKruise 1.2, which is a CNCF Sandbox level project.","date":"2022-06-07T00:00:00.000Z","formattedDate":"June 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":7.065,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"openkruise-1.2","title":"OpenKruise v1.2, new PersistentPodState feature to achieve IP retention","authors":["FillZpp"],"tags":["release"]},"prevItem":{"title":"OpenKruise v1.3, New Custom Pod Probe Capabilities and Significant Performance Improvements for Large-Scale Clusters","permalink":"/blog/openkruise-1.3"},"nextItem":{"title":"OpenKruise v1.1, features enhanced, improve performance in large-scale clusters","permalink":"/blog/openkruise-1.1"}},"content":"We\u2019re pleased to announce the release of OpenKruise 1.2, which is a CNCF Sandbox level project.\\n\\n[OpenKruise](https://openkruise.io) is an extended component suite for Kubernetes, which mainly focuses on application automations, such as deployment, upgrade, ops and availability protection. Mostly features provided by OpenKruise are built primarily based on CRD extensions. They can work in pure Kubernetes clusters without any other dependences.\\n\\n## What\'s new?\\n\\nIn release v1.2, OpenKruise provides a new CRD named `PersistentPodState`, some new fields of CloneSet status and lifecycle hook, and optimization of PodUnavailableBudget.\\n\\nHere we are going to introduce some changes of it.\\n\\n### 1. New CRD and Controller: PersistentPodState\\n\\nWith the development of cloud native, more and more companies start to deploy stateful services (e.g., Etcd, MQ) using Kubernetes. K8S StatefulSet is a workload for managing stateful services, and it considers the deployment characteristics of stateful services in many aspects.\\nHowever, StatefulSet persistent only limited pod state, such as Pod Name is ordered and unchanging, PVC persistence, and can not cover other states, e.g. Pod IP retention, priority scheduling to previously deployed Nodes, etc. Typical Cases:\\n\\n- **Service Discovery Middleware services are exceptionally sensitive to the Pod IP after deployment, requiring that the IP cannot be changed.**\\n\\n- **Database services persist data to the host disk, and changes to the Node to which they belong will result in data loss.**\\n\\nIn response to the above description, by customizing `PersistentPodState` CRD, Kruise is able to persistent other states of the Pod, such as \\"IP Retention\\".\\n\\nAn object of `PersistentPodState` may look like this:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: PersistentPodState\\nmetadata:\\n  name: echoserver\\n  namespace: echoserver\\nspec:\\n  targetRef:\\n    # Native k8s or kruise StatefulSet\\n    # only support StatefulSet\\n    apiVersion: apps.kruise.io/v1beta1\\n    kind: StatefulSet\\n    name: echoserver\\n  # required node affinity. As follows, Pod rebuild will force deployment to the same zone\\n  requiredPersistentTopology:\\n    nodeTopologyKeys:\\n      failure-domain.beta.kubernetes.io/zone[,other node labels]\\n  # preferred node affinity. As follows, Pod rebuild will preferred deployment to the same node\\n  preferredPersistentTopology:\\n    - preference:\\n        nodeTopologyKeys:\\n          kubernetes.io/hostname[,other node labels]\\n      # int [1, 100]\\n      weight: 100\\n```\\n\\n\\"IP Retention\\" should be a common requirement for K8S deployments of stateful services. It does not mean \\"Specified Pod IP\\", but requires that the Pod IP does not change after the first deployment, either by service release or by machine eviction.\\nTo achieve this, we need the K8S network component to support Pod IP retention and the ability to keep the IP as unchanged as possible.\\nIn this article, we have modified the Host-local plugin in the flannel network component so that it can achieve the effect of keeping the Pod IP unchanged under the same Node.\\nRelated principles will not be stated here, please refer to the code: [host-local](https://github.com/openkruise/samples/tree/master/containernetworking/plugins).\\n\\nIP retention seems to be supported by the network component, how is it related with PersistentPodState?\\nWell, there are some limitations to the implementation of \\"Pod IP unchanged\\" by network components. For example, flannel can only support the same Node to keep the Pod IP unchanged.\\nHowever, the most important feature of K8S scheduling is \\"uncertainty\\", so \\"how to ensure that Pods are rebuilt and scheduled to the same Node\\" is the problem that PersistentPodState solves.\\n\\nAlso you can add the annotations below on your StatefulSet or Advanced StatefulSet, to let Kruise automatically create a `PersistentPodState` object for the StatefulSet. So you don\'t have to create it manually.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: StatefulSet\\nmetadata:\\n  annotations:\\n    # auto generate PersistentPodState\\n    kruise.io/auto-generate-persistent-pod-state: \\"true\\"\\n    # preferred node affinity, As follows, Pod rebuild will preferred deployment to the same node\\n    kruise.io/preferred-persistent-topology: kubernetes.io/hostname[,other node labels]\\n    # required node affinity, As follows, Pod rebuild will force deployment to the same zone\\n    kruise.io/required-persistent-topology: failure-domain.beta.kubernetes.io/zone[,other node labels]\\n```\\n\\n### 2. CloneSet percentage partition calculation changed (**breaking**), and a new field in its status\\n\\nPreviously, CloneSet calculates its `partition` with round up if it is a percentage value, which means\\neven you set `partition` to be a percentage less than `100%`, it might update no Pods to the new revision.\\nFor example, the real partition of a CloneSet with `replicas=8` and `partition=90%` will be calculated as\\n`8` because of `8 * 90%` with round up, so it will not update any Pod.\\nThis is a little confused, especially when we are using a rollout component like Kruise Rollout or Argo.\\n\\n**So since v1.2, CloneSet will make sure there is at lease one Pod should be updated when `partition` is a percentage less than `100%`, unless the CloneSet has `replicas <= 1`.**\\n\\nHowever, it might be difficult for users to understand this arithmetic, but they have to known the expected\\nupdated number of Pods after a percentage partition was set.\\n\\nSo we also provide a new field `expectedUpdatedReplicas` in CloneSet status, which directly shows the\\nexpected updated number of Pods based on the given `partition`.\\nUsers only have to compare `status.updatedReplicas >= status.expectedUpdatedReplicas`\\nto decide whether their CloneSet has finished rolling out new revision under partition restriction or not.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: CloneSet\\nspec:\\n  replicas: 8\\n  updateStrategy:\\n    partition: 90%\\nstatus:\\n  replicas: 8\\n  expectedUpdatedReplicas: 1\\n  updatedReplicas: 1\\n  updatedReadyReplicas: 1\\n```\\n\\n### 3. Able to mark Pod not-ready for lifecycle hook\\n\\nKruise has already provided lifecycle hook in previous versions. CloneSet and Advanced StatefulSet support both\\nPreDelete and InPlaceUpdate hooks, while Advanced DaemonSet only supports PreDelete hook.\\n\\nPreviously, the hooks only pause the operation and allow users to do something\\n(for example remove pod from service endpoints) during Pod deleting and before/after in-place update.\\nBut the Pod is probably Ready during the hook state, so that removing it from some custom service implementation\\nmay break the rule of Kubernetes that we\'d better only remove NotReady Pods from the endpoints.\\n\\nSo that a new field has been added into the lifecycle hook, `markPodNotReady` indicates the hooked Pod should be\\nmarked as NotReady or not.\\n\\n```go\\ntype LifecycleStateType string\\n\\n// Lifecycle contains the hooks for Pod lifecycle.\\ntype Lifecycle struct \\n    // PreDelete is the hook before Pod to be deleted. \\n    PreDelete *LifecycleHook `json:\\"preDelete,omitempty\\"` \\n    // InPlaceUpdate is the hook before Pod to update and after Pod has been updated. \\n    InPlaceUpdate *LifecycleHook `json:\\"inPlaceUpdate,omitempty\\"`\\n}\\n\\ntype LifecycleHook struct {\\n    LabelsHandler     map[string]string `json:\\"labelsHandler,omitempty\\"`\\n    FinalizersHandler []string          `json:\\"finalizersHandler,omitempty\\"`\\n\\t\\n    /**********************  FEATURE STATE: 1.2.0 ************************/\\n    // MarkPodNotReady = true means:\\n    // - Pod will be set to \'NotReady\' at preparingDelete/preparingUpdate state.\\n    // - Pod will be restored to \'Ready\' at Updated state if it was set to \'NotReady\' at preparingUpdate state.\\n    // Default to false.\\n    MarkPodNotReady bool `json:\\"markPodNotReady,omitempty\\"`\\n    /*********************************************************************/\\t\\n}\\n```\\n\\nFor PreDelete hook, it will set Pod to be NotReady during PreparingDelete state if `markPodNotReady` is true,\\nand the Pod can not be changed back to normal even if the `replicas` is increased again.\\n\\nFor InPlaceUpdate hook, it will set Pod to be NotReady during PreparingUpdate state if `markPodNotReady` is true,\\nand the NotReady condition will be removed during Updated state.\\n\\n### 4. PodUnavailableBudget supports any custom workloads and performance optimization\\n\\nKubernetes offers PodDisruptionBudget to help users run highly available applications even when you introduce frequent voluntary disruptions,\\nbut it can only constrain the voluntary disruption triggered by the Eviction API.\\n\\nIn voluntary disruption scenarios, PodUnavailableBudget can achieve the effect of preventing application disruption or SLA degradation, which greatly improves the high availability of application services.\\nIt can not only protect application Pods from eviction but also deletion, in-place update and other operations that could make Pods not ready.\\n\\nPreviously, PodUnavailableBudget only supports some specific workloads like CloneSet and Deployment. But it can not recognize unknown workloads that\\nmay be defined by users themself.\\n\\nSince v1.2 release, PodUnavailableBudget has supported any custom workloads to protect their Pods from unavailable operations.\\nAll you have to do is to declare scale subresource for those custom workloads.\\n\\nIt looks like this in CRD:\\n\\n```yaml\\n    subresources:\\n      scale:\\n        labelSelectorPath: .status.labelSelector\\n        specReplicasPath: .spec.replicas\\n        statusReplicasPath: .status.replicas\\n```\\n\\nBut if you are using kubebuilder or operator-sdk to generate your project, one line comment on your workload struct will be fine:\\n\\n```go\\n// +kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.labelSelector\\n```\\n\\nBesides, PodUnavailableBudget also optimizes its performance for large-scale clusters by disable DeepCopy from client list.\\n\\n### 5. Other changes\\n\\nFor more changes, their authors and commits, you can read the [Github release](https://github.com/openkruise/kruise/releases).\\n\\n## Get Involved\\n\\nWelcome to get involved with OpenKruise by joining us in Github/Slack/DingTalk/WeChat.\\nHave something you\u2019d like to broadcast to our community?\\nShare your voice at our [Bi-weekly community meeting (Chinese)](https://shimo.im/docs/gXqmeQOYBehZ4vqo), or through the channels below:\\n\\n- Join the community on [Slack](https://kubernetes.slack.com/channels/openkruise) (English).\\n- Join the community on DingTalk: Search GroupID `23330762` (Chinese).\\n- Join the community on WeChat (new): Search User `openkruise` and let the robot invite you (Chinese)."},{"id":"openkruise-1.1","metadata":{"permalink":"/blog/openkruise-1.1","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2022-03-29-release-1.1.md","source":"@site/blog/2022-03-29-release-1.1.md","title":"OpenKruise v1.1, features enhanced, improve performance in large-scale clusters","description":"We\u2019re pleased to announce the release of Kubernetes 1.1, which is a CNCF Sandbox level project.","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":7.9,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"openkruise-1.1","title":"OpenKruise v1.1, features enhanced, improve performance in large-scale clusters","authors":["FillZpp"],"tags":["release"]},"prevItem":{"title":"OpenKruise v1.2, new PersistentPodState feature to achieve IP retention","permalink":"/blog/openkruise-1.2"},"nextItem":{"title":"OpenKruise v1.0, Reaching New Peaks of application automation","permalink":"/blog/openkruise-1.0"}},"content":"We\u2019re pleased to announce the release of Kubernetes 1.1, which is a CNCF Sandbox level project.\\n\\n[OpenKruise](https://openkruise.io) is an extended component suite for Kubernetes, which mainly focuses on application automations, such as deployment, upgrade, ops and availability protection. Mostly features provided by OpenKruise are built primarily based on CRD extensions. They can work in pure Kubernetes clusters without any other dependences.\\n\\n## What\'s new?\\n\\nIn release v1.1, OpenKruise optimizes some existing features, and improves its performance in large-scale clusters.\\nHere we are going to introduce some changes of it.\\n\\nNote that OpenKruise v1.1 bumps Kubernetes dependencies to v1.22, which means we can use new fields of up to K8s v1.22 in Pod template of workloads like CloneSet and Advanced StatefulSet.\\nBut OpenKruise can still be used in Kubernetes cluster >= 1.16 version.\\n\\n### 1. Keep containers order for in-place update\\n\\nIn the release v1.0 we published last year, OpenKruise has intruduced [Container Launch Priority](/docs/user-manuals/containerlaunchpriority/),\\nwhich supports to define different priorities for containers in a Pod and keeps their start order during Pod creation.\\n\\nBut in v1.0, it can only control the order in Pod creation. If you try to update the containers in-place, they will be updated at the same time.\\n\\nRecently, the community has discussed with some companies such as LinkedIn and get more input from the users.\\nIn some scenarios, the containers in Pod may have special relationship, for example base-container should firstly update its configuration before app-container update,\\nor we have to forbid multiple containers updating together to avoid log-container losing the logs of app-container.\\n\\nSo, OpenKruise supports in-place update with container priorities since v1.1.\\n\\n\\nThere is no extra options, just make sure containers have their launch priorities since Pod creation.\\nIf you modify them **both in once in-place update**, Kruise will firstly update the containers with higher priority.\\nThen Kruise will not update the containers with lower priority util the higher one has updated successfully.\\n\\n**The in-place udpate here includes both modification of image and env from metadata, read the [concept doc](/docs/core-concepts/inplace-update) for more details**\\n\\n- For pods without container launch priorities, no guarantees of the execution order during in-place update multiple containers.\\n- For pods with container launch priorities:\\n  - keep execution order during in-place update multiple containers with different priorities.\\n  - no guarantees of the execution order during in-place update multiple containers with the same priority.\\n\\nFor example, we have the CloneSet that includes two containers with different priorities:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: CloneSet\\nmetadata:\\n  ...\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      annotations:\\n        app-config: \\"... config v1 ...\\"\\n    spec:\\n      containers:\\n      - name: sidecar\\n        env:\\n        - name: KRUISE_CONTAINER_PRIORITY\\n          value: \\"10\\"\\n        - name: APP_CONFIG\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: metadata.annotations[\'app-config\']\\n      - name: main\\n        image: main-image:v1\\n  updateStrategy:\\n    type: InPlaceIfPossible\\n```\\n\\nWhen we update the CloneSet to change `app-config` annotation and image of main container, which means both sidecar and main containers need to update,\\nKruise will firstly in-place update pods that recreates sidecar container with the new env from annotation.\\n\\nAt this moment, we can find the `apps.kruise.io/inplace-update-state` annotation in updated Pod and see its value:\\n\\n```json\\n{\\n  \\"revision\\": \\"{CLONESET_NAME}-{HASH}\\",         // the target revision name of this in-place update\\n  \\"updateTimestamp\\": \\"2022-03-22T09:06:55Z\\",    // the start time of this whole update\\n  \\"nextContainerImages\\": {\\"main\\": \\"main-image:v2\\"},                // the next containers that should update images\\n  // \\"nextContainerRefMetadata\\": {...},                            // the next containers that should update env from annotations/labels\\n  \\"preCheckBeforeNext\\": {\\"containersRequiredReady\\": [\\"sidecar\\"]},  // the pre-check must be satisfied before the next containers can update\\n  \\"containerBatchesRecord\\":[\\n    {\\"timestamp\\":\\"2022-03-22T09:06:55Z\\",\\"containers\\":[\\"sidecar\\"]}  // the first batch of containers that have updated (it just means the spec of containers has updated, such as images in pod.spec.container or annotaions/labels, but dosn\'t mean the real containers on node have been updated completely)\\n  ]\\n}\\n```\\n\\nWhen the sidecar container has been updated successfully, Kruise will update the next main container. Finally, you will find the `apps.kruise.io/inplace-update-state` annotation looks like:\\n\\n```json\\n{\\n  \\"revision\\": \\"{CLONESET_NAME}-{HASH}\\",\\n  \\"updateTimestamp\\": \\"2022-03-22T09:06:55Z\\",\\n  \\"lastContainerStatuses\\":{\\"main\\":{\\"imageID\\":\\"THE IMAGE ID OF OLD MAIN CONTAINER\\"}},\\n  \\"containerBatchesRecord\\":[\\n    {\\"timestamp\\":\\"2022-03-22T09:06:55Z\\",\\"containers\\":[\\"sidecar\\"]},\\n    {\\"timestamp\\":\\"2022-03-22T09:07:20Z\\",\\"containers\\":[\\"main\\"]}\\n  ]\\n}\\n```\\n\\nUsually, users only have to care about the `containerBatchesRecord` to make sure the containers are updated in different batches. If the Pod is blocking during in-place update, you should check the `nextContainerImages/nextContainerRefMetadata` and see if the previous containers in `preCheckBeforeNext` have been updated successfully and ready.\\n\\n### 2. StatefulSetAutoDeletePVC\\n\\nSince Kubernetes v1.23, the upstream StatefulSet has supported StatefulSetAutoDeletePVC feature, it **controls if and how PVCs are deleted during the lifecycle of a StatefulSet**, refer to [this doc](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention).\\n\\nSo, Advanced StatefulSet has rebased this feature from upstream, which also requires you to enable `StatefulSetAutoDeletePVC` feature-gate during install/upgrade Kruise.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1beta1\\nkind: StatefulSet\\nspec:\\n  ...\\n  persistentVolumeClaimRetentionPolicy:  # optional\\n    whenDeleted: Retain | Delete\\n    whenScaled: Retain | Delete\\n```\\n\\nOnce enabled, there are two policies you can configure for each StatefulSet:\\n\\n- `whenDeleted`: configures the volume retention behavior that applies when the StatefulSet is deleted.\\n- `whenScaled`: configures the volume retention behavior that applies when the replica count of the StatefulSet is reduced; for example, when scaling down the set.\\n\\nFor each policy that you can configure, you can set the value to either `Delete` or `Retain`.\\n\\n- `Retain` (default): PVCs from the `volumeClaimTemplate` are not affected when their Pod is deleted. This is the behavior before this new feature.\\n- `Delete`: The PVCs created from the `volumeClaimTemplate` are deleted for each Pod affected by the policy. With the `whenDeleted` policy all PVCs from the `volumeClaimTemplate` are deleted after their Pods have been deleted. With the `whenScaled` policy, only PVCs corresponding to Pod replicas being scaled down are deleted, after their Pods have been deleted.\\n\\nNote that:\\n\\n1. StatefulSetAutoDeletePVC only deletes PVCs created by `volumeClaimTemplate` instead of the PVCs created by user or related to StatefulSet Pod.\\n2. The policies only apply when Pods are being removed due to the StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to the node where the new Pod is about to launch.\\n\\n### 3. Advanced DaemonSet refactor, lifecycle hook\\n\\nThe behavior of Advanced DaemonSet used to be a little different with the upstream controller,\\nsuch as it required extra configuration to choose whether not-ready and unschedulable nodes should be handled,\\nwhich makes users confused and hard to understand.\\n\\nIn release v1.1, we have refactored Advanced DaemonSet to make it rebase with upstream.\\nNow, the default behavior of Advanced DaemonSet should be same with the upstream DaemonSet,\\nwhich means users can conveniently modify the `apiVersion` field to convert a built-in DaemonSet to Advanced DaemonSet.\\n\\nMeanwhile, we also add lifecycle hook for Advanced DaemonSet.\\nCurrently it supports preDelete hook, which allows users to do something (for example check node resources) before Pod deleting.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: DaemonSet\\nspec:\\n  ...\\n  # define with label\\n  lifecycle:\\n    preDelete:\\n      labelsHandler:\\n        example.io/block-deleting: \\"true\\"\\n```\\n\\nWhen Advanced DaemonSet delete a Pod (including scale in and recreate update):\\n- Delete it directly if no lifecycle hook definition or Pod not matched preDelete hook\\n- Otherwise, Advanced DaemonSet will firstly update Pod to `PreparingDelete` state and wait for user controller to remove the label/finalizer and Pod not matched preDelete hook\\n\\n### 4. Improve performance by disable DeepCopy\\n\\nBy default, when we are writing Operator/Controller with controller-runtime and use the Client interface in `sigs.k8s.io/controller-runtime/pkg/client` to get/list typed objects,\\nit will always get objects from Informer. That\'s known by most people.\\n\\nBut what\'s many people don\'t know, is that controller-runtime will firstly deep copy all the objects got from Informer and then return the copied objects.\\n\\nThis design aims to avoid developers directly modifying the objects in Informer.\\nAfter DeepCopy, no matter how developers modify the objected returned by get/list, it will not change the objects in Informer, which are only synced by ListWatch from kube-apiserver.\\n\\nHowever, in some large-scale clusters, mutliple controllers of OpenKruise and their workers are reconciling together, which may bring so many DeepCopy operations.\\nFor example, there are a lot of application CloneSets and some of them have managed thousands of Pods,\\nthen each worker will list all Pod of the CloneSet during Reconcile and there exists multiple workers.\\nIt brings CPU and Memory pressure to kruise-manager and even sometimes makes it Out-Of-Memory.\\n\\nSo I have submitted and merged [DisableDeepCopy feature](https://github.com/kubernetes-sigs/controller-runtime/pull/1274) in upstream,\\nwhich contains in controller-runtime >= v0.10 version.\\nIt allows developers to specify some resource types that will directly return the objects from Informer without DeepCopy during get/list.\\n\\nFor example, we can add cache options when initialize `Manager` in `main.go` to avoid DeepCopy for Pod objects.\\n\\n```go\\n    mgr, err := ctrl.NewManager(cfg, ctrl.Options{\\n\\t\\t...\\n\\t\\tNewCache: cache.BuilderWithOptions(cache.Options{\\n\\t\\t\\tUnsafeDisableDeepCopyByObject: map[client.Object]bool{\\n\\t\\t\\t\\t&v1.Pod{}: true,\\n\\t\\t\\t},\\n\\t\\t}),\\n\\t})\\n```\\n\\nBut in Kruise v1.1, we re-implement [Delegating Client](https://github.com/openkruise/kruise/blob/master/pkg/util/client/delegating_client.go) instead of using the feature of controller-runtime.\\nIt allows developers to avoid DeepCopy with `DisableDeepCopy ListOption` in any list places, which is more flexible.\\n\\n```go\\n    if err := r.List(context.TODO(), &podList, client.InNamespace(\\"default\\"), utilclient.DisableDeepCopy); err != nil {\\n\\t\\treturn nil, nil, err\\n\\t}\\n```\\n\\n### 5. Other changes\\n\\nFor more changes, their authors and commits, you can read the [Github release](https://github.com/openkruise/kruise/releases).\\n\\n## Get Involved\\n\\nWelcome to get involved with OpenKruise by joining us in Github/Slack/DingTalk/WeChat.\\nHave something you\u2019d like to broadcast to our community?\\nShare your voice at our [Bi-weekly community meeting (Chinese)](https://shimo.im/docs/gXqmeQOYBehZ4vqo), or through the channels below:\\n\\n- Join the community on [Slack](https://kubernetes.slack.com/channels/openkruise) (English).\\n- Join the community on DingTalk: Search GroupID `23330762` (Chinese).\\n- Join the community on WeChat (new): Search User `openkruise` and let the robot invite you (Chinese)."},{"id":"openkruise-1.0","metadata":{"permalink":"/blog/openkruise-1.0","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-12-13-release-1.0.md","source":"@site/blog/2021-12-13-release-1.0.md","title":"OpenKruise v1.0, Reaching New Peaks of application automation","description":"We\u2019re pleased to announce the release of Kubernetes 1.0, which is a CNCF Sandbox level project.","date":"2021-12-13T00:00:00.000Z","formattedDate":"December 13, 2021","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":6.505,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"openkruise-1.0","title":"OpenKruise v1.0, Reaching New Peaks of application automation","authors":["FillZpp"],"tags":["release"]},"prevItem":{"title":"OpenKruise v1.1, features enhanced, improve performance in large-scale clusters","permalink":"/blog/openkruise-1.1"},"nextItem":{"title":"WorkloadSpread - Interpretation for OpenKruise v0.10.0 new feature","permalink":"/blog/workloadspread"}},"content":"We\u2019re pleased to announce the release of Kubernetes 1.0, which is a CNCF Sandbox level project.\\n\\n[OpenKruise](https://openkruise.io) is an extended component suite for Kubernetes, which mainly focuses on application automations, such as deployment, upgrade, ops and availability protection. Mostly features provided by OpenKruise are built primarily based on CRD extensions. They can work in pure Kubernetes clusters without any other dependences.\\n\\n![openkruise-features|center|450x400](/img/blog/2021-12-13-release-1.0/features-en.png)\\n\\nOverall, OpenKruise currently provides features in these areas:\\n\\n- **Application workloads**: Enhanced strategies of deploy and upgrade for stateless/stateful/daemon applications, such as in-place update, canary/flowing upgrade.\\n- **Sidecar container management**: supports to define sidecar container alone, which means it can inject sidecar containers, upgrade them with no effect on application containers and even hot upgrade.\\n- **Enhanced operations**: such as restart containers in-place, pre-download images on specific nodes, keep containers launch priority in a Pod, distribute one resource to multiple namespaces.\\n- **Application availability protection**: protect availability for applications that deployed in Kubernetes.\\n\\n## What\'s new?\\n\\n### 1. InPlace Update for environments\\n\\n*Author: [@FillZpp](https://github.com/FillZpp)*\\n\\nOpenKruise has supported **InPlace Update** since very early version, mostly for workloads like CloneSet and Advanced StatefulSet. Comparing to recreate Pods during upgrade, in-place update only has to modify the fields in existing Pods.\\n\\n![inplace-update-comparation|center|450x400](/img/docs/core-concepts/inplace-update-comparation.png)\\n\\nAs the picture shows above, we only modify the `image` field in Pod during in-place update. So that:\\n\\n- Avoid additional cost of *scheduling*, *allocating IP*, *allocating and mounting volumes*.\\n- Faster image pulling, because of we can re-use most of image layers pulled by the old image and only to pull several new layers.\\n- When a container is in-place updating, the other containers in Pod will not be affected and remain running.\\n\\nHowever, OpenKruise only supports to in-place update `image` field in Pod and has to recreate Pods if other fields need to update. All the way through, more and more users hope OpenKruise could support in-place update more fields such as `env` -- which is hard to implement, for it is limited by kube-apiserver.\\n\\nAfter our unremitting efforts, OpenKruise finally support in-place update environments via Downward API since version v1.0. Take the CloneSet YAML below as an example, user has to set the configuration in annotation and write a env from it. After that, he just needs to modify the annotation value when changing the configuration. Kruise will restart all containers with env from the annotation in such Pod to enable the new configuration.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: CloneSet\\nmetadata:\\n  ...\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      annotations:\\n        app-config: \\"... the real env value ...\\"\\n    spec:\\n      containers:\\n      - name: app\\n        env:\\n        - name: APP_CONFIG\\n          valueFrom:\\n            fieldRef:\\n              fieldPath: metadata.annotations[\'app-config\']\\n  updateStrategy:\\n    type: InPlaceIfPossible\\n```\\n\\n*At the same time, we have removed the limit of `imageID` for in-place update, which means you can update a new image with the same imageID to the old image.*\\n\\nFor more details please read [documentation](/docs/core-concepts/inplace-update).\\n\\n### 2. Distribute resources over multiple namespaces\\n\\n*Author: [@veophi](https://github.com/veophi)*\\n\\nFor the scenario, where the namespace-scoped resources such as Secret and ConfigMap need to be distributed or synchronized to different namespaces, the native k8s currently only supports manual distribution and synchronization by users one-by-one, which is very inconvenient. \\n\\nTypical examples: \\n- When users want to use the imagePullSecrets capability of SidecarSet, they must repeatedly create corresponding Secrets in relevant namespaces, and ensure the correctness and consistency of these Secret configurations;\\n- When users want to configure some common environment variables, they probably need to distribute ConfigMaps to multiple namespaces, and the subsequent modifications of these ConfigMaps might require synchronization among these namespaces.\\n\\nTherefore, in the face of these scenarios that require the resource distribution and **continuously synchronization across namespaces**, we provide a tool, namely **ResourceDistribution**, to do this automatically. \\n\\nCurrently, ResourceDistribution supports the two kind resources --- **Secret & ConfigMap**. \\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: ResourceDistribution\\nmetadata:\\n  name: sample\\nspec:\\n  resource:\\n    apiVersion: v1\\n    kind: ConfigMap\\n    metadata:\\n      name: game-demo\\n    data:\\n      ...\\n  targets:\\n  \\tnamespaceLabelSelector:\\n      ...\\n    # or includedNamespaces, excludedNamespaces\\n```\\n\\nSo you can see ResourceDistribution is a kind of **cluster-scoped CRD**, which is mainly composed of two fields: **`resource` and `targets`**.\\n- `resource` is a **complete** and **correct** resource structure in YAML style.\\n- `targets` indicates the target namespaces that the resource should be distributed into.\\n\\nFor more details please read [documentation](/docs/user-manuals/resourcedistribution).\\n\\n### 3. Container launch priority\\n\\n*Author: [@Concurrensee](https://github.com/Concurrensee)*\\n\\nContainers in a same Pod in it might have dependence, which means the application in one container runs depending on another container. For example:\\n\\n1. Container A has to start first. Container B can start only if A is already running.\\n2. Container B has to exit first. Container A can stop only if B has already exited.\\n\\nCurrently, the sequences of containers start and stop are controlled by Kubelet.\\nKubernetes used to have a KEP, which plans to add a type field for container to identify the priority of start and stop. However, it has been refused because of sig-node thought it may bring a huge change to code.\\n\\nSo OpenKruise provides a feature named **Container Launch Priority**, which helps user control the sequence of containers start in a Pod.\\n\\n1. User only has to put the annotation `apps.kruise.io/container-launch-priority: Ordered` in a Pod, then Kruise will ensure all containers in this Pod should be started by the sequence of `pod.spec.containers` list.\\n2. If you want to customize the launch sequence, you can add `KRUISE_CONTAINER_PRIORITY` environment in container. The range of the value is `[-2147483647, 2147483647]`. The container with higher priority will be guaranteed to start before the others with lower priority.\\n\\nFor more details please read [documentation](/docs/user-manuals/containerlaunchpriority).\\n\\n### 4. `kubectl-kruise` commandline tool\\n\\n*Author: [@hantmac](https://github.com/hantmac)*\\n\\nOpenKruise used to provide SDK like `kruise-api` and `client-java` for some programming languages, which can be imported into users\' projects. On the other hand, some users also need to operate the workload resources with commandline in test environment.\\n\\nHowever, the `rollout`, `set image` commands in original `kubectl` can only work for built-in workloads, such as Deployment and StatefulSet.\\n\\nSo, OpenKruise now provide a commandline tool named `kubectl-kruise`, which is a standard plugin of `kubectl` and can work for OpenKruise workload types.\\n\\n```bash\\n# rollout undo cloneset\\n$ kubectl kruise rollout undo cloneset/nginx\\n\\n#  rollout status advanced statefulset\\n$ kubectl kruise rollout status statefulsets.apps.kruise.io/sts-demo\\n\\n# set image of a cloneset\\n$ kubectl kruise set image cloneset/nginx busybox=busybox nginx=nginx:1.9.1\\n```\\n\\nFor more details please read [documentation](/docs/cli-tool/kubectl-plugin).\\n\\n### 5. Other changes\\n\\n**CloneSet:**\\n- Add `maxUnavailable` field in `scaleStrategy` to support rate limiting of scaling up.\\n- Mark revision stable when all pods updated to it, won\'t wait all pods to be ready.\\n\\n**WorkloadSpread:**\\n- Manage the pods that have created before WorkloadSpread.\\n- Optimize the update and retry logic for webhook injection.\\n\\n**Advanced DaemonSet:**\\n- Support in-place update Daemon Pod.\\n- Support progressive annotation to control if pods creation should be limited by partition.\\n\\n**SidecarSet:**\\n- Fix SidecarSet filter active pods.\\n- Add `SourceContainerNameFrom` and `EnvNames` fields in `transferenv` to make the container name flexible and the list shorter.\\n\\n**PodUnavailableBudget:**\\n- Add no pub-protection annotation to skip validation for the specific Pod.\\n- PodUnavailableBudget controller watches workload replicas changed.\\n\\n**NodeImage:**\\n- Add `--nodeimage-creation-delay` flag to delay NodeImage creation after Node ready.\\n\\n**UnitedDeployment:**\\n- Fix pod NodeSelectorTerms length 0 when UnitedDeployment NodeSelectorTerms is nil.\\n\\n**Other optimization:**\\n- kruise-daemon list and watch pods using protobuf.\\n- Export cache resync args and defaults to be 0 in chart value.\\n- Fix http checker reloading after webhook certs updated.\\n- Generate CRDs with original controller-tools and markers.\\n\\n## Get Involved\\n\\nWelcome to get involved with OpenKruise by joining us in Github/Slack/DingTalk/WeChat.\\nHave something you\u2019d like to broadcast to our community?\\nShare your voice at our [Bi-weekly community meeting (Chinese)](https://shimo.im/docs/gXqmeQOYBehZ4vqo), or through the channels below:\\n\\n- Join the community on [Slack](https://kubernetes.slack.com/channels/openkruise) (English).\\n- Join the community on DingTalk: Search GroupID `23330762` (Chinese).\\n- Join the community on WeChat: Search User `openkruise` and let the robot invite you (Chinese)."},{"id":"workloadspread","metadata":{"permalink":"/blog/workloadspread","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-09-22-workloadspread.md","source":"@site/blog/2021-09-22-workloadspread.md","title":"WorkloadSpread - Interpretation for OpenKruise v0.10.0 new feature","description":"Background","date":"2021-09-22T00:00:00.000Z","formattedDate":"September 22, 2021","tags":[{"label":"workload","permalink":"/blog/tags/workload"},{"label":"workloadspread","permalink":"/blog/tags/workloadspread"},{"label":"multi-domain","permalink":"/blog/tags/multi-domain"}],"readingTime":11.875,"truncated":false,"authors":[{"name":"GuangLei Cao","title":"Contributor of OpenKruise","url":"https://github.com/BoltsLei","imageURL":"https://github.com/BoltsLei.png","key":"BoltsLei"},{"name":"Weixiang Sun","title":"Member of OpenKruise","url":"https://github.com/veophi","imageURL":"https://github.com/veophi.png","key":"veophi"}],"frontMatter":{"slug":"workloadspread","title":"WorkloadSpread - Interpretation for OpenKruise v0.10.0 new feature","authors":["BoltsLei","veophi"],"tags":["workload","workloadspread","multi-domain"]},"prevItem":{"title":"OpenKruise v1.0, Reaching New Peaks of application automation","permalink":"/blog/openkruise-1.0"},"nextItem":{"title":"OpenKruise 0.10.0, New features of multi-domain management, application protection","permalink":"/blog/openkruise-0.10.0"}},"content":"## Background\\n\\nDeploying an application in different zones, different hardware types, and even different clusters and cloud vendors is becoming a very common requirement with the development of cloud native techniques. \\nFor examples, these are some cases:\\n1. Cases about disaster tolerant:\\n  - Application pods is scattered according to the nodes to avoid stacking.\\n  - Application pods is scattered according to available zones.\\n  - Different nodes/zones/domains require different scale of pods.\\n2. Cases about cost control:\\n  - People deploy an applications preferentially to their own resource pool, and then deployed to elastic resource pool, such as ECI on Aliyun and Fragate on AWS, when own resources are insufficient. When shrinking, the elastic node is preferred to shrink to save cost.\\n\\nIn the most of the cases, people always split their application into multiple workloads (such as several `Deployment`) to deploy. However\uff0cthis solution often requires manual management by SRE team, or a deeply customized PAAS to support the careful management of multiple workloads for this one application.\\n\\nIn order to solve this problem, WorkloadSpread feature has been proposed in version v0.10.0 OpenKruise. It can support multi-kind of workloads, such as `Deployment`, `Replicaset`, `Job`, and `Cloneset`, to manage the partition deployment or elastic scaling. The application scenario and implementation principle of WorkloadSpread will be introduced in detail below to help users better understand this feature.\\n\\n---\\n\\n## Introduction\\n\\nMore details about WorkloadSpread can be found in [Offical Document](https://openkruise.io/docs/user-manuals/workloadspread). \\n\\nIn short, WorkloadSpread can distribute pods of a workload to different types of nodes according to certain rules, so as to meet the above fragmentation and elasticity scenarios. WorkloadSpread is non-invasive, \\"plug and play\\", and can be effective for stock workloads.\\n\\n---\\n\\n## Comparison with related works\\n\\nLet\'s make a simple comparison with some related works in the community.\\n\\n### \u300c1\u300dPod Topology Spread Constrains\\n[Pod topology spread constraints](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/) is a solution provided by Kubernetes community. It can horizontally scatter pods according to topology key. The scheduler will select the node that matches the conditions according to the configuration if users defined this rule.\\n\\nSince Pod Topology Spread is evenly dispersed, it **cannot** support exact customized partition number and proportion configuration. Furthermore, the distribution of pods will be destroyed when scaling down. \\nUsing WorkloadSpread can avoid these problems.\\n\\n### \u300c2\u300dUnitedDeploymen\\n[UnitedDeployment](https://openkruise.io/docs/user-manuals/uniteddeployment) is a solution provided by the OpenKruise community. It can manage pods in multiple regions by creating and managing multiple workloads.\\n\\nUnitedDeployment supports the requirements of fragmentation and flexibility very well. But, it is a new workload, and the use cost and migration costs will be relatively high, whereas WorkloadSpread is a lightweight solution, which only needs to apply a simple configuration to associate the workload.\\n\\n---\\n\\n## Use Case\\n\\nIn the section, I will list some application scenarios of WorkloadSpread and give corresponding configurations to help users quickly understand the WorkloadSpread feature.\\n\\n### \u300c1\u300dDeploy 100 pods to normal node pool, rest pods to elastic node pool\\n\\n![case-1](../static/img/blog/2021-09-22-workloadspread/case-1.jpg)\\n\\n```yaml\\nsubsets:\\n- name: subset-normal\\n  maxReplicas: 100\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: app.deploy/zone\\n      operator: In\\n      values:\\n      - normal\\n- name: subset-elastic \\n# maxReplicas==nil means no limit for replicas\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: app.deploy/zone\\n      operator: In\\n      values:\\n      - elastic\\n```\\n\\nWhen the workload has less than 100 replicas, all pods will be deployed to the normal node pool, and more than 100 are deployed to the elastic node pool. When scaling down, the pods on the elastic node will be deleted first.\\n\\nSince workload spread limits the distribution of workload, but does not invade workload. Users can also dynamically adjust the number of replicas according to the resource load in combination with HPA. \\n\\nIn this way, it will be automatically scheduled to the elastic node pool when receiving peak flow, and give priority to releasing the resources in the elastic resource pool when the peak gone.\\n\\n\\n### \u300c1\u300dDeploy pods to normal node pool first, to elastic resource pool when normal node pool is insufficient\\n\\n![case-2](../static/img/blog/2021-09-22-workloadspread/case-2.jpg)\\n\\n```yaml\\nscheduleStrategy:\\n  type: Adaptive\\n  adaptive:\\n    rescheduleCriticalSeconds: 30\\n    disableSimulationSchedule: false\\nsubsets:\\n- name: subset-normal\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: app.deploy/zone\\n      operator: In\\n      values:\\n      - normal\\n- name: subset-elastic\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: app.deploy/zone\\n      operator: In\\n      values:\\n      - elastic\\n```\\n\\nBoth subsets have no limit on the number of replicas, and the `Adaptive` rescheduling policy are enabled. \\nThe goal is to preferentially deploy to the normal node pool. When normal resources are insufficient, webhook will select elastic nodes through simulated scheduling. When the pod in the normal node pool is in the pending state and exceeds the 30s threshold, the WorkloadSpread controller will delete the pod to trigger pod reconstruction, and the new pod will be scheduled to the elastic node pool. During volume reduction, the pod on the elastic node is also preferentially reduced to save costs for users.\\n\\n### \u300c3\u300dScatter to 3 zones, the scale is 1:1:3\\n\\n![case-3](../static/img/blog/2021-09-22-workloadspread/case-3.jpg)\\n\\n```yaml\\nsubsets:\\n- name: subset-a\\n  maxReplicas: 20%\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: topology.kubernetes.io/zone\\n      operator: In\\n      values:\\n      - zone-a\\n- name: subset-b\\n  maxReplicas: 20%\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: topology.kubernetes.io/zone\\n      operator: In\\n      values:\\n      - zone-b\\n- name: subset-c\\n  maxReplicas: 60%\\n  requiredNodeSelectorTerm:\\n    matchExpressions:\\n    - key: topology.kubernetes.io/zone\\n      operator: In\\n      values:\\n      - zone-c   \\n```\\n\\nWorkloadSpread ensures that the pods are scheduled according to the defined proportion when scaling up and down.\\n\\n### \u300c4\u300dConfigures different resource quotas on different CPU architecture\\n\\n![case-4](../static/img/blog/2021-09-22-workloadspread/case-4.jpg)\\n\\n```yaml\\nsubsets:\\n- name: subset-x86-arch\\n  # maxReplicas...\\n  # requiredNodeSelectorTerm...\\n  patch:\\n    metadata:\\n      labels:\\n        resource.cpu/arch: x86\\n    spec: \\n      containers:\\n      - name: main\\n        resources:\\n          limits:\\n            cpu: \\"500m\\"\\n            memory: \\"800Mi\\"\\n- name: subset-arm-arch\\n  # maxReplicas...\\n  # requiredNodeSelectorTerm...\\n  patch:\\n    metadata:\\n      labels:\\n        resource.cpu/arch: arm\\n    spec: \\n      containers:\\n      - name: main\\n        resources:\\n          limits:\\n            cpu: \\"300m\\"\\n            memory: \\"600Mi\\"\\n```\\n\\nFrom the above example, we have patched different labels and container `resources` for the pods of two subsets, which is convenient for us to manage the pod more finely. When workload pods are distributed on nodes of different CPU architectures, configure different resource quotas to make better use of hardware resources.\\n\\n---\\n\\n## Implementation\\n\\nWorkloadSpread is a pure bypass elastic/topology control solution. Users only need to create a corresponding WorkloadSpread config for their Deployment/Cloneset/Job/ReplicaSet Workloads. There is no need to change the them, and users will be no additional cost to use the WorkloadSpread.\\n\\n![arch](../static/img/blog/2021-09-22-workloadspread/arch.jpg)\\n\\n### \u300c1\u300d How to decide the priority when scaling up?\\nMultiple subsets are defined in WorkloadSpread, and each subset represents a logical domain. Users can freely define subsets according to node configuration, hardware type, zone, etc. In particular, we defined the priority of subsets:\\n\\n- The priority is defined from high to low in the order from front to back, for example `subset[i]` has higher priority than `subset[j]` if `i < j`.\\n\\n- The pods will be scheduled to the subsets with higher priority first.\\n  \\n### \u300c2\u300d How to decide the priority when scaling down?\\nTheoretically, the bypass solution of WorkloadSpread cannot interfere with the scaling logic in the workload controller.\\n\\nHowever, this problem has been solved in the near future. Through the unremitting efforts (feedback) of users, k8s since version 1.21, it has been supported for ReplicaSet (deployment) to specify the \\"deletion cost\\" of the pods by setting the annotation `controller.kubernetes.io/pod-deletion-cost`: the higher the deletion cost, the lower the priority of deletion.\\n\\nSince version v0.9.0 OpenKruise, the deletion cost feature has been supported in cloneset.\\n\\n**Therefore, the WorkloadSpread controller controls the scaling down order of the pods by adjusting their deletion cost.**\\n\\nFor example, an WorkloadSpread associated a CloneSet with 10 replicas is as follows:\\n\\n```yaml\\n  subsets:\\n  - name: subset-a\\n    maxReplicas: 8\\n  - name: subset-b\\n```\\n\\nThen the deletion cost value and deletion order are as follows:\\n- 8 pods in subset-a will have 200 deletion cost;\\n- 2 pods in subset-b will have 100 deletion cost, and will be deleted first;\\n  \\nIf user modify WorkloadSpread as:\\n\\n```yaml\\n  subsets:\\n  - name: subset-a\\n    maxReplicas: 5 # 8->5, \\n  - name: subset-b\\n```\\n\\nThen the deletion cost value and deletion order will also changed as follows:\\n- 5 pods in subset-a will have 200 deletion cost;\\n- 3 pods in subset-a will have -100 deletion cost, and will be deleted first;\\n- 2 pods in subset-b will have 100 deletion cost;\\n\\nIn this way, workload can preferentially scale down those pods that exceed the subset `maxReplicas` limit.\\n\\n### \u300c3\u300d How to solve the counting problems?\\nHow to ensure that webhook injects pod rules in strict accordance with the priority order of subset and the number of maxReplicas is a key problem at the implementation of WorkloadSpread.\\n\\n#### 3.1 solving concurrency consistency problem\\nSine there may be several kruise-controller-manager pods and lots of webhook Goroutines to process the same WorkloadSpread, the concurrency consistency problem must exist.\\n\\nIn the status of WorkloadSpread, there are the `subsetStatuses` field corresponding to each subset. The `missingReplicas` field in it indicates the number of pods required by the subset, and - 1 indicates that there is no quantity limit (`subset.maxReplicas == nil`).\\n\\n```yaml\\nspec:\\n  subsets:\\n  - name: subset-a\\n    maxReplicas: 1\\n  - name: subset-b\\n  # ...\\nstatus:\\n  subsetStatuses:\\n  - name: subset-a\\n    missingReplicas: 1\\n  - name: subset-b\\n    missingReplicas: -1\\n  # ...\\n```\\n\\nWhen webhook receives a pod create request:\\n1. Find a suitable subset with `missingReplicas` greater than `0` or equals to `-1`  according to the subset order.\\n2. After finding a suitable subset, if `missingReplicas` is greater than `0`, subtract `1` first and try to update the WorkloadSpread status.\\n3. If the update is successful, inject the rules defined by the subset into the pod.\\n4. If the update fails, get the WorkloadSpread again to get the latest status, and return to step 1 (there is a certain limit on the number of retries).\\n\\nSimilarly, when webhook receives a pod delete or eviction request, `MisingReplicas` will add `1` to missingreplicas and update it.\\n\\nThere is no doubt that we are using optimistic locks to solve update conflicts. **However, it is not appropriate to only use optimistic locks**, because workload will create a large number of pods in parallel, and APIServer will send many pod create requests to webhook in an instant, resulting in a lot of conflicts in parallel processing.\\nAs we all know, optimistic lock is not suitable for too many conflicts, because the retry cost of solving conflicts is very high. To this end, we also added a WorkloadSpread level mutex to limit parallel processing to serial processing. There is a new problem in adding mutex locks, that is, after the current root obtains the lock, it is very likely that the WorkloadSpread obtained from infomer is not up-to-date, and will conflict as well. Therefore, after updating the WorkloadSpread, the Goroutine caches the latest WorkloadSpread and then releases the lock, so that the new Goroutine can directly get the latest WorkloadSpread from the cache after obtaining the lock. Of course, in the case of multiple webhooks, we still need to combine the optimistic lock mechanism to solve the conflict.\\n\\n#### 3.2 solving data consistency problem\\n\\nSo, is the `missingReplicas` field controlled by the webhook? The answer is **NO**, because:\\n\\n1. The pod create request received by webhook may not really succeed in the end (for example, pod is illegal or fails in subsequent quota verification).\\n\\n2. The pod delete/eviction request received by webhook may not really succeed in the end (for example, it is intercepted by PDB, PUB, etc.).\\n\\n3. There are always various possibilities in k8s, leading to the end or disappearance of the pods without going through webhook (for example, phase enters succeeded/failed, or ETCD data is lost, etc.).\\n\\n4. At the same time, this is not in line with the end state oriented design concept.\\n\\nTherefore, the WorkloadSpread status is controlled by webhook in collaboration with the controller:\\n\\n- Webhook requests link interception in pod create/delete/ eviction, and modifies the `missingReplicas`.\\n\\n- At the same time, the controller\'s reconcile will also get all pods under the current workload, classify them according to the subset, and update `missingReplicas` to the actual missing quantity.\\n\\n- From the above analysis, it is likely that there is a delay for the controller to obtain the pod from the informer, so we also added the `creatingPods` map in the status. When the pod is injected at webhook, the key will be recorded as pod name and value are timestamp to the map, and the controller maintains the real `missingReplicas` in combination with the map. Similarly, there is also a `deleteingPods` map to record the delete/eviction event of the pod.\\n\\n### \u300c4\u300dHow to do if pod schedule failed?\\nThe configuration of reschedule strategy is supported in WorkloadSpread. By default, the type is fixed, that is, the pod is scheduled to the corresponding subset according to the sequence of each subset and the `maxReplicas` limit.\\n\\nHowever, in real scenarios, many times, the resources of subset may not fully meet the number of maxReplicas due to some reasons, such as insufficient resources. Users need a more flexible reschedule strategy.\\n\\nThe adaptive capabilities provided by WorkloadSpread are logically divided into two types:\\n\\n1. SimulationSchedule: scheduling records exists in informer, so we want to simulate the scheduling of pods in webhook. That is, simple filtering is performed through `nodeSelector`/`Affinity`, Tolerances, and basic resources resources. (not applicable to virtual-kubelet)\\n\\n2. Reschedule: After scheduling the pod to a subset, if the scheduling failure exceeds the rescheduleCriticalSeconds time, mark the subset as unscheduled temporarily, and delete the pod to trigger reconstruction. By default, unscheduled will be reserved for 5min, that is, pod creation within 5min will skip this subset.\\n\\n---\\n\\n## Conclusion\\n\\nWorkloadSpread combines some existing features of Kubernetes to give workload the ability of elastic and multi-domain deployment in the form of bypass. We hope that users can reduce workload deployment complexity by using WorkloadSpread and effectively reduce costs by taking advantage of its elastic scalability.\\n\\nAt present, WorkloadSpread is applied to some project in Alibaba, and adjustments in the use will be fed back to the community in time. In the future, there are some new capability plans for WorkloadSpread, such as managing the existing pods, supporting batch workloads, and even using label to match the pod across different workloads. Some of these capabilities need to actually consider the needs and scenarios of community users. I hope you can participate in kruise community, mention Issues and PRs, help users solve the problems of more cloud native deployment, and build a better community.\\n\\n---\\n## Reference\\n- WorkloadSpread: https://openkruise.io/docs/user-manuals/workloadspread\\n- Pod Topology Spread Constrains: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\\n- UnitedDeployment: https://openkruise.io/docs/user-manuals/uniteddeployment"},{"id":"openkruise-0.10.0","metadata":{"permalink":"/blog/openkruise-0.10.0","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-09-06-release-0.10.0.md","source":"@site/blog/2021-09-06-release-0.10.0.md","title":"OpenKruise 0.10.0, New features of multi-domain management, application protection","description":"On Sep 6th, 2021, OpenKruise released the latest version v0.10.0, with new features, such as WorkloadSpread and PodUnavailableBudget. This article provides an overview of this new version.","date":"2021-09-06T00:00:00.000Z","formattedDate":"September 6, 2021","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":4.67,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"openkruise-0.10.0","title":"OpenKruise 0.10.0, New features of multi-domain management, application protection","authors":["FillZpp"],"tags":["release"]},"prevItem":{"title":"WorkloadSpread - Interpretation for OpenKruise v0.10.0 new feature","permalink":"/blog/workloadspread"},"nextItem":{"title":"OpenKruise 0.9.0, SidecarSet Helps Mesh Container Hot Upgrade","permalink":"/blog/sidecarset-hotupdate"}},"content":"On Sep 6th, 2021, OpenKruise released the latest version v0.10.0, with new features, such as WorkloadSpread and PodUnavailableBudget. This article provides an overview of this new version.\\n\\n## WorkloadSpread\\n\\nWorkloadSpread can distribute Pods of workload to different types of Node according to some polices, which empowers single workload the abilities for\\nmulti-domain deployment and elastic deployment.\\n\\nSome common policies include:\\n- fault toleration spread (for example, spread evenly among hosts, az, etc)\\n- spread according to the specified ratio (for example, deploy Pod to several specified az according to the proportion)\\n- subset management with priority, such as\\n  - deploy Pods to ecs first, and then deploy to eci when its resources are insufficient.\\n  - deploy a fixed number of Pods to ecs first, and the rest Pods are deployed to eci.\\n- subset management with customization, such as\\n  - control how many pods in a workload are deployed in different cpu arch\\n  - enable pods in different cpu arch to have different resource requirements\\n\\nThe feature of WorkloadSpread is similar with UnitedDeployment in OpenKruise community. Each WorkloadSpread defines multi-domain\\ncalled `subset`. Each domain may provide the limit to run the replicas number of pods called `maxReplicas`.\\nWorkloadSpread injects the domain configuration into the Pod by Webhook, and it also controls the order of scale in and scale out.\\n\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: WorkloadSpread\\nmetadata:\\n  name: workloadspread-demo\\nspec:\\n  targetRef:\\n    apiVersion: apps/v1 | apps.kruise.io/v1alpha1\\n    kind: Deployment | CloneSet\\n    name: workload-xxx\\n  subsets:\\n  - name: subset-a\\n    requiredNodeSelectorTerm:\\n      matchExpressions:\\n      - key: topology.kubernetes.io/zone\\n        operator: In\\n        values:\\n        - zone-a\\n    maxReplicas: 10 | 30%\\n  - name: subset-b\\n    requiredNodeSelectorTerm:\\n      matchExpressions:\\n      - key: topology.kubernetes.io/zone\\n        operator: In\\n        values:\\n        - zone-b\\n```\\n\\nThe WorkloadSpread is related to a Workload via `targetRef`. When a Pod is created by the Workload, it will be injected topology policies by Kruise according to the rules in WorkloadSpread.\\n\\nNote that WorkloadSpread uses [Pod Deletion Cost](https://kubernetes.io/docs/reference/labels-annotations-taints/#pod-deletion-cost) to control the priority of scale down. So:\\n\\n- If the Workload type is CloneSet, it already supports the feature.\\n- If the Workload type is Deployment or ReplicaSet, it requires your Kubernetes version >= 1.22.\\n\\nAlso you have to enable `WorkloadSpread` feature-gate when you install or upgrade Kruise.\\n\\n## PodUnavailableBudget\\n\\nKubernetes offers [Pod Disruption Budget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) to help you run highly available applications even when you introduce frequent [voluntary disruptions](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/).\\nPDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. However, it can only constrain the voluntary disruption triggered by the [Eviction API](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#eviction-api).\\nFor example, when you run kubectl drain, the tool tries to evict all of the Pods on the Node you\'re taking out of service.\\n\\nIn the following voluntary disruption scenarios, there are still business disruption or SLA degradation situations:\\n1. The application owner update deployment\'s pod template for general upgrading, while cluster administrator drain nodes to scale the cluster down(learn about [Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)).\\n2. The middleware team is using SidecarSet to rolling upgrade the sidecar containers of the cluster, e.g. ServiceMesh envoy, while HPA triggers the scale-down of business applications.\\n3. The application owner and middleware team release the same Pods at the same time based on OpenKruise cloneSet, sidecarSet in-place upgrades\\n\\nIn voluntary disruption scenarios, PodUnavailableBudget can achieve the effect of preventing application disruption or SLA degradation, which greatly improves the high availability of application services.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: PodUnavailableBudget\\nmetadata:\\n  name: web-server-pub\\n  namespace: web\\nspec:\\n  targetRef:\\n    apiVersion: apps/v1 | apps.kruise.io/v1alpha1\\n    kind: Deployment | CloneSet | StatefulSet | ...\\n    name: web-server\\n  # selector label query over pods managed by the budget\\n  # selector and TargetReference are mutually exclusive, targetRef is priority to take effect.\\n  # selector is commonly used in scenarios where applications are deployed using multiple workloads,\\n  # and targetRef is used for protection against a single workload.\\n# selector:\\n#   matchLabels:\\n#     app: web-server\\n  # maximum number of Pods unavailable for the current cloneset, the example is cloneset.replicas(5) * 60% = 3\\n  # maxUnavailable and minAvailable are mutually exclusive, maxUnavailable is priority to take effect\\n  maxUnavailable: 60%\\n  # Minimum number of Pods available for the current cloneset, the example is cloneset.replicas(5) * 40% = 2\\n# minAvailable: 40%\\n```\\n\\nYou have to enable the feature-gates when install or upgrade Kruise:\\n\\n- PodUnavailableBudgetDeleteGate: protect Pod deletion or eviction.\\n- PodUnavailableBudgetUpdateGate: protect Pod update operations, such as in-place update.\\n\\n## CloneSet supports scaledown priority by Spread Constraints\\n\\nWhen `replicas` of a CloneSet decreased, it has the arithmetic to choose Pods and delete them.\\n\\n1. Node unassigned < assigned\\n2. PodPending < PodUnknown < PodRunning\\n3. Not ready < ready\\n4. **Lower pod-deletion cost < higher pod-deletion-cost**\\n5. **Higher spread rank < lower spread rank**\\n6. Been ready for empty time < less time < more time\\n7. Pods with containers with higher restart counts < lower restart counts\\n8. Empty creation time pods < newer pods < older pods\\n\\n\\"4\\" has provided in Kruise v0.9.0 and it is also used by WorkloadSpread to control the Pod deletion. **\\"5\\" is added in Kruise v0.10.0 to sort Pods by their Topology Spread Constraints during scaledown.**\\n\\n## Advanced StatefulSet supports scaleup with rate limit\\n\\nTo avoid a large amount of failed Pods after user created an incorrect Advanced StatefulSet, Kruise add a `maxUnavailable` field into its `scaleStrategy`.\\n\\n```yaml\\napiVersion: apps.kruise.io/v1beta1\\nkind: StatefulSet\\nspec:\\n  # ...\\n  replicas: 100\\n  scaleStrategy:\\n    maxUnavailable: 10% # percentage or absolute number\\n```\\n\\nWhen the field is set, Advanced StatefulSet will guarantee that the number of unavailable Pods should not bigger than the strategy number during Pod creation.\\n\\nNote that the feature can only be used in StatefulSet with `podManagementPolicy=Parallel`.\\n\\n## More\\n\\nFor more changes, please refer to the [release page](https://github.com/openkruise/kruise/releases) or [ChangeLog](https://github.com/openkruise/kruise/blob/master/CHANGELOG.md)."},{"id":"sidecarset-hotupdate","metadata":{"permalink":"/blog/sidecarset-hotupdate","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-06-10-sidecarset-hot-update-0.9.0.md","source":"@site/blog/2021-06-10-sidecarset-hot-update-0.9.0.md","title":"OpenKruise 0.9.0, SidecarSet Helps Mesh Container Hot Upgrade","description":"OpenKruise is an open source management suite developed by Alibaba Cloud for cloud native application automation. It is currently a Sandbox project hosted under the Cloud Native Computing Foundation (CNCF). Based on years of Alibaba\'s experience in container and cloud native technologies, OpenKruise is a Kubernetes-based standard extension component that has been widely used in the Alibaba internal production environment, together with technical concepts and best practices for large-scale Internet scenarios.","date":"2021-06-10T00:00:00.000Z","formattedDate":"June 10, 2021","tags":[{"label":"workload","permalink":"/blog/tags/workload"},{"label":"sidecar","permalink":"/blog/tags/sidecar"},{"label":"istio","permalink":"/blog/tags/istio"},{"label":"mosn","permalink":"/blog/tags/mosn"},{"label":"HotUpgrade","permalink":"/blog/tags/hot-upgrade"}],"readingTime":7.425,"truncated":false,"authors":[{"name":"Mingshan Zhao","title":"Member of OpenKruise","url":"https://github.com/zmberg","imageURL":"https://github.com/zmberg.png","key":"zmberg"}],"frontMatter":{"slug":"sidecarset-hotupdate","title":"OpenKruise 0.9.0, SidecarSet Helps Mesh Container Hot Upgrade","authors":["zmberg"],"tags":["workload","sidecar","istio","mosn","HotUpgrade"]},"prevItem":{"title":"OpenKruise 0.10.0, New features of multi-domain management, application protection","permalink":"/blog/openkruise-0.10.0"},"nextItem":{"title":"OpenKruise 0.9.0, Supports Pod Restart and Deletion Protection","permalink":"/blog/openkruise-0.9.0"}},"content":"OpenKruise is an open source management suite developed by Alibaba Cloud for cloud native application automation. It is currently a Sandbox project hosted under the Cloud Native Computing Foundation (CNCF). Based on years of Alibaba\'s experience in container and cloud native technologies, OpenKruise is a Kubernetes-based standard extension component that has been widely used in the Alibaba internal production environment, together with technical concepts and best practices for large-scale Internet scenarios.\\n\\nOpenKruise released v0.8.0 on March 4, 2021, with enhanced SidecarSet capabilities, especially for log management of Sidecar.\\n\\n# Background - How to Upgrading Mesh Containers Independently\\nSidecarSet is a workload provided by Kruise to manage sidecar containers. Users can complete **automatic injection** and **independent upgrades** conveniently using SidecarSet.\\n\\nBy default, sidecar upgrade will first stop the old container and start a new one. This method is particularly suitable for sidecar containers that do not affect Pod service availability, such as log collection agents. However, for many proxies or sidecar containers for runtime, such as Istio Envoy, this upgrade method does not work. Envoy functions as a Proxy container in the Pod to handle all traffic. If users restart in this scenario to upgrade directly, the service availability of the Pod will be affected. Therefore, the release and capacity of the application should be taken into consideration. The sidecar release cannot be independent of the application.\\n\\n![how update mesh sidecar](../static/img/blog/2021-06-10-sidecarset-hot-update/how_update_mesh.png)\\n\\nTens of thousands of pods in Alibaba Group communicate with each other based on Service Mesh. Mesh container upgrades may make business pods unavailable. Therefore, the upgrade of the mesh containers hinders the iteration of Service Mesh. To address this scenario, we worked with the Service Mesh team to implement the hot upgrade capability of the mesh container. This article focuses on the important role SidecarSet is playing during the implementation of the hot upgrade capability of mesh containers.\\n\\n# SidecarSet Helps Lossless Hot Upgrade of Mesh Containers\\nMesh containers cannot perform direct in-place upgrades like the log collection class container. The mesh container must provide services without interruption, but an independent upgrade will make the mesh service unavailable for some time. Some well-known mesh services in the community, such as Envoy and Mosn, provide smooth upgrade capabilities by default. However, these upgrade methods cannot be integrated properly with cloud-native, and Kubernetes does not have an upgrade solution for such sidecar containers.\\n\\n**OpenKruise SidecarSet provides the sidecar hot upgrade mechanism for the mesh container**. Thus, lossless Mesh container hot upgrade can be implemented in a cloud-native manner.\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: hotupgrade-sidecarset\\nspec:\\n  selector:\\n    matchLabels:\\n      app: hotupgrade\\n  containers:\\n  - name: sidecar\\n    image: openkruise/hotupgrade-sample:sidecarv1\\n    imagePullPolicy: Always\\n    lifecycle:\\n      postStart:\\n        exec:\\n          command:\\n          - /bin/sh\\n          - /migrate.sh\\n    upgradeStrategy:\\n      upgradeType: HotUpgrade\\n      hotUpgradeEmptyImage: openkruise/hotupgrade-sample:empty\\n```\\n- upgradeType: \u201cHotUpgrade\u201d indicates this type of sidecar container, which is hot upgrade.\\n- hotUpgradeEmptyImage: When performing hot upgrade on sidecar containers, businesses need to provide an empty container for container switchover. The Empty container has the same configuration as the sidecar container (except for the image address), such as command, lifecycle, and probe.\\n\\nThe SidecarSet hot upgrade mechanism includes two steps: injection of Sidecar containers of the hot upgrade type and Mesh container smooth upgrade.\\n\\n## Inject Sidecar Containers of the Hot Upgrade Type\\nFor Sidecar containers of the hot upgrade type, two containers will be injected by SidercarSet Webhook when creating the Pod:\\n- {sidecar.name}-1: As shown in the following figure, envoy-1 represents a running sidecar container, for example, envoy:1.16.0.\\n- {sidecar.name}-2: As shown in the following figure, envoy-2 represents the \u201chotUpgradeEmptyImage\u201d container provided by the business, for example, empty:1.0.\\n\\n![inject sidecar](../static/img/blog/2021-06-10-sidecarset-hot-update/inject_sidecar.png)\\n\\nThis Empty container does not have any practical work while running the Mesh container.\\n\\n## Smooth Mesh Container Upgrade\\nThe hot upgrade process is divided into three steps:\\n1. **Upgrade:** Replace the Empty container with the sidecar container of the latest version, for example, ```envoy-2.Image = envoy:1.17.0```\\n2. **Migration:** Run the \u201cPostStartHook\u201d script of the sidecar container to upgrade the mesh service smoothly\\n3. **Reset:** After the mesh service is upgraded, replace the sidecar container of the earlier version with an Empty container, for example, ```envoy-1.Image = empty:1.0```\\n\\n![update sidecar](../static/img/blog/2021-06-10-sidecarset-hot-update/update_sidecar.png)\\n\\nThe preceding three steps represent the entire process of the hot upgrade. If multiple hot upgrades on a Pod are required, users only need to repeat the three steps listed above.\\n\\n## Core Logic of Migration\\nThe SidecarSet hot upgrade mechanism completes the mesh container switching and provides the coordination mechanism (```PostStartHook```) for containers of old and new versions. However, this is only the first step. The Mesh container also needs to provide the ```PostStartHook``` script to upgrade the mesh service smoothly (please see the preceding migration process), such as Envoy hot restart and Mosn lossless restart.\\n\\nMesh containers generally provide external services by listening to a fixed port. The migration process of mesh containers can be summarized as: pass ListenFD through UDS, stop Accept, and start drainage. For mesh containers that do not support hot restart, you can follow this process to modify the mesh containers. The logic is listed below:\\n\\n![migration](../static/img/blog/2021-06-10-sidecarset-hot-update/migration.png)\\n\\n## Migration Demo\\nDifferent mesh containers provide different services and have different internal implementation logics, so the specific Migrations are also different. The preceding logic only presents some important points, with hopes to benefit everyone in need. We have also provided a [hot upgrade Migration Demo](https://github.com/openkruise/samples) on GitHub for reference. Next, we will introduce some of the key codes:\\n1. **Consultation Mechanism**\\nFirst, users must **check whether it is the first startup or hot upgrade smooth migration** to start the Mesh container. Kruise injects two environment variables called ```SIDECARSET_VERSION``` and ```SIDECARSET_VERSION_ALT``` to two sidecar containers to reduce the communication cost of the mesh container. The two environment variables determine whether it is running the hot upgrade process and whether the current sidecar container version is new or old.\\n```\\n// return two parameters:\\n// 1. (bool) indicates whether it is hot upgrade process\\n// 2. (bool ) when isHotUpgrading=true, the current sidecar is newer or older\\nfunc isHotUpgradeProcess() (bool, bool) {\\n  // Version of the current sidecar container\\n  version := os.Getenv(\\"SIDECARSET_VERSION\\")\\n  // Version of the peer sidecar container\\n  versionAlt := os.Getenv(\\"SIDECARSET_VERSION_ALT\\")\\n  // If the version of the peer sidecar container is \\"0\\", hot upgrade is not underway\\n  if versionAlt == \\"0\\" {\\n    return false, false\\n  }\\n  // Hot upgrade is underway\\n  versionInt, _ := strconv.Atoi(version)\\n  versionAltInt, _ := strconv.Atoi(versionAlt)\\n  // version is of int type and monotonically increases, which means the version value of the new-version container will be greater\\n  return true, versionInt > versionAltInt\\n}\\n```\\n\\n2. **ListenFD Migration**\\nUse the Unix Domain Socket to migrate ListenFD between containers. This step is also a critical step in the hot upgrade. The code example is listed below:\\n```\\n  // For code conciseness, all failures will not be captured\\n\\n  /* The old sidecar migrates ListenFD to the new sidecar through Unix Domain Socket */\\n  // tcpLn *net.TCPListener\\n  f, _ := tcpLn.File()\\n  fdnum := f.Fd()\\n  data := syscall.UnixRights(int(fdnum))\\n  // Establish a connection with the new sidecar container through Unix Domain Socket\\n  raddr, _ := net.ResolveUnixAddr(\\"unix\\", \\"/dev/shm/migrate.sock\\")\\n  uds, _ := net.DialUnix(\\"unix\\", nil, raddr)\\n  // Use UDS to send ListenFD to the new sidecar container\\n  uds.WriteMsgUnix(nil, data, nil)\\n  // Stop receiving new requests and start the drainage phase, for example, http2 GOAWAY\\n  tcpLn.Close()\\n\\n  /* The new sidecar receives ListenFD and starts to provide external services */\\n  // Listen to UDS\\n  addr, _ := net.ResolveUnixAddr(\\"unix\\", \\"/dev/shm/migrate.sock\\")\\n  unixLn, _ := net.ListenUnix(\\"unix\\", addr)\\n  conn, _ := unixLn.AcceptUnix()\\n  buf := make([]byte, 32)\\n  oob := make([]byte, 32)\\n  // Receive ListenFD\\n  _, oobn, _, _, _ := conn.ReadMsgUnix(buf, oob)\\n  scms, _ := syscall.ParseSocketControlMessage(oob[:oobn])\\n  if len(scms) > 0 {\\n    // Parse FD and convert to *net.TCPListener\\n    fds, _ := syscall.ParseUnixRights(&(scms[0]))\\n    f := os.NewFile(uintptr(fds[0]), \\"\\")\\n    ln, _ := net.FileListener(f)\\n    tcpLn, _ := ln.(*net.TCPListener)\\n    // Start to provide external services based on the received Listener. The http service is used as an example\\n    http.Serve(tcpLn, serveMux)\\n  }\\n\\n```\\n\\n# Successful Mesh Container Hot Upgrade Cases\\n[Alibaba Cloud Service Mesh](https://www.alibabacloud.com/product/servicemesh) (ASM) provides a fully managed service mesh platform compatible with open-source Istio service mesh from the community. Currently, ASM implements the Sidecar hot upgrade capability (Beta) in the data plane based on the hot upgrade capability of OpenKruise SidecarSet. Users can upgrade the data plane version of service mesh without affecting applications.\\n\\nIn addition to hot upgrades, ASM supports capabilities, such as configuration diagnosis, operation audit, log access, monitoring, and service registration, to improve the user experience of service mesh. You are welcome to try it out!\\n\\n# Summary\\nThe hot upgrade of mesh containers in cloud-native has always been an urgent but thorny problem. The solution in this article is only one exploration of Alibaba Group, giving feedback to the community with hopes of encouraging better ideas. We also welcome everyone to participate in the [OpenKruise](https://github.com/openkruise/kruise) community. Together, we can build mature Kubernetes application management, delivery, and extension capabilities that can be applied to more large-scale, complex, and high-performance scenarios."},{"id":"openkruise-0.9.0","metadata":{"permalink":"/blog/openkruise-0.9.0","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-05-20-release-0.9.0.md","source":"@site/blog/2021-05-20-release-0.9.0.md","title":"OpenKruise 0.9.0, Supports Pod Restart and Deletion Protection","description":"On May 20, 2021, OpenKruise released the latest version v0.9.0, with new features, such as Pod restart and resource cascading deletion protection. This article provides an overview of this new version.","date":"2021-05-20T00:00:00.000Z","formattedDate":"May 20, 2021","tags":[{"label":"release","permalink":"/blog/tags/release"}],"readingTime":12.315,"truncated":false,"authors":[{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"openkruise-0.9.0","title":"OpenKruise 0.9.0, Supports Pod Restart and Deletion Protection","authors":["FillZpp"],"tags":["release"]},"prevItem":{"title":"OpenKruise 0.9.0, SidecarSet Helps Mesh Container Hot Upgrade","permalink":"/blog/sidecarset-hotupdate"},"nextItem":{"title":"OpenKruise 0.8.0, A Powerful Tool for Sidecar Container Management","permalink":"/blog/sidecarset"}},"content":"On May 20, 2021, OpenKruise released the latest version v0.9.0, with new features, such as Pod restart and resource cascading deletion protection. This article provides an overview of this new version.\\n\\n## Pod Restart and Recreation\\n\\nRestarting container is a necessity in daily operation and a common technical method for recovery. In the native Kubernetes, the container granularity is inoperable. Pod, as the minimum operation unit, can only be created or deleted.\\n\\nSome may ask: *why do users still need to pay attention to the operation such as container restart in the cloud-native era? Aren\'t the services the only thing for users to focus on in the ideal Serverless model?*\\n\\nTo answer this question, we need to see the differences between cloud-native architecture and traditional infrastructures. In the era of traditional physical and virtual machines, multiple application instances are deployed and run on one machine, but the lifecycles of the machine and applications are separated. Thus, application instance restart may only require a `systemctl` or `supervisor` command but not the restart of the entire machine. However, in the era of containers and cloud-native, the lifecycle of the application is bound to that of the Pod container. In other words, under normal circumstances, one container only runs one application process, and one Pod provides services for only one application instance.\\n\\nDue to these restrictions, current native Kubernetes provides no API for the container (application) restart for upper-layer services. OpenKruise v0.9.0 supports restarting containers in a single Pod, compatible with standard Kubernetes clusters of version 1.16 or later. After installing or upgrading OpenKruise, users only need to create a `ContainerRecreateRequest` (CRR) object to initiate a restart process. The simplest YAML file is listed below:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: ContainerRecreateRequest\\nmetadata:\\n  namespace: pod-namespace\\n  name: xxx\\nspec:\\n  podName: pod-name\\n  containers:\\n  - name: app\\n  - name: sidecar\\n```\\n\\nThe value of namespace must be the same as the namespace of the Pod to be operated. The name can be set as needed. The `podName` in the spec clause indicates the Pod name. The containers indicate a list that specifies one or more container names in the Pod to restart.\\n\\nIn addition to the required fields above, CRR also provides a variety of optional restart policies:\\n\\n```yaml\\nspec:\\n  # ...\\n  strategy:\\n    failurePolicy: Fail\\n    orderedRecreate: false\\n    terminationGracePeriodSeconds: 30\\n    unreadyGracePeriodSeconds: 3\\n    minStartedSeconds: 10\\n  activeDeadlineSeconds: 300\\n  ttlSecondsAfterFinished: 1800\\n```\\n\\n- `failurePolicy`: Values: Fail or Ignore. Default value: Fail. If any container stops or fails to recreate, CRR ends immediately.\\n- `orderedRecreate`: Default value: false. Value true indicates when the list contains multiple containers, the new container will only be recreated after the previous recreation is finished.\\n- `terminationGracePeriodSeconds`: The time for the container to gracefully exit. If this parameter is not specified, the time defined for the Pod is used.\\n- `unreadyGracePeriodSeconds`: Set the Pod to the unready state before recreation and wait for the time expiration to execute recreation.\\n  - `Note`: This feature needs the feature-gate `KruisePodReadinessGate` to be enabled, which will inject a readinessGate when a Pod is created. Otherwise, only the pods created by the OpenKruise workload are injected with readinessGate by default. It means only these Pods can use the `unreadyGracePeriodSeconds` parameter during the CRR recreation.\\n- `minStartedSeconds`: The minimal period that the new container remains running to judge whether the container is recreated successfully.\\n- `activeDeadlineSeconds`: The expiration period set for CRR execution to mark as ended (unfinished container will be marked as failed.)\\n- `ttlSecondsAfterFinished`: The period after which the CRR will be deleted automatically after the execution ends.\\n\\n**How it works under the hood:** After it is created, a CRR is processed by the kruise-manager. Then, it will be sent to the kruise-daemon (contained by the node where Pod resides) for execution. The execution process is listed below:\\n\\n1. If `preStop` is specified for a Pod, the kruise-daemon will first call the CRI to run the command specified by `preStop` in the container.\\n2. If no `preStop` exists or `preStop` execution is completed, the kruise-daemon will call the CRI to stop the container.\\n3. When the kubelet detects the container exiting, it creates a new container with an increasing \\"serial number\\" and starts it. `postStart` will be executed at the same time.\\n4. When the kruise-daemon detects the start of the new container, it reports to CRR that the restart is completed.\\n\\n![ContainerRecreateRequest](/img/docs/user-manuals/containerrecreaterequest.png)\\n\\nThe container \\"serial number\\" corresponds to the `restartCount` reported by kubelet in the Pod status. Therefore, the `restartCount` of the Pod increases after the container is restarted. Temporary files written to the `rootfs` in the old container will be lost due to the container recreation, but data in the volume mount remains.\\n\\n## Cascading Deletion Protection\\n\\nThe level triggered automation of Kubernetes is a double-edged sword. It brings declarative deployment capabilities to applications while potentially enlarging the influence of mistakes at a final-state scale. For example, with the cascading deletion mechanism, once an owning resource is deleted under normal circumstances (non-orphan deletion), all owned resources associated will be deleted by the following rules:\\n\\n1. If a CRD is deleted, all its corresponding CR will be cleared.\\n2. If a namespace is deleted, all resources in this namespace, including Pods, will be cleared.\\n3. If a workload (Deployment, StatefulSet, etc) is deleted, all Pods under it will be cleared.\\n\\nDue to failures caused by cascading deletion, we have heard many complaints from Kubernetes users and developers in the community. It is unbearable for any enterprise to mistakenly delete objects at such a large scale in the production environment.\\n\\nTherefore, in OpenKruise v0.9.0, we applied the feature of cascading deletion protection to community in the hope of ensuring stability for more users. If you want to use this feature in the current version, the feature-gate of `ResourcesDeletionProtection` needs to be explicitly enabled when installing or upgrading OpenKruise.\\n\\nA label of `policy.kruise.io/delete-protection` can be given on the resource objects that require protection. Its value can be the following two things:\\n\\n- **Always**: The object cannot be deleted unless the label is removed.\\n- **Cascading**: The object cannot be deleted if any subordinate resources are available.\\n\\nThe following table lists the supported resource types and cascading relationships:\\n\\n| Kind                        | Group                  | Version            | **Cascading** judgement                            |\\n| --------------------------- | ---------------------- | ------------------ | ----------------------------------------------------\\n| `Namespace`                 | core                   | v1                 | whether there is active Pods in this namespace     |\\n| `CustomResourceDefinition`  | apiextensions.k8s.io   | v1beta1, v1        | whether there is existing CRs of this CRD          |\\n| `Deployment`                | apps                   | v1                 | whether the replicas is 0                          |\\n| `StatefulSet`               | apps                   | v1                 | whether the replicas is 0                          |\\n| `ReplicaSet`                | apps                   | v1                 | whether the replicas is 0                          |\\n| `CloneSet`                  | apps.kruise.io         | v1alpha1           | whether the replicas is 0                          |\\n| `StatefulSet`               | apps.kruise.io         | v1alpha1, v1beta1  | whether the replicas is 0                          |\\n| `UnitedDeployment`          | apps.kruise.io         | v1alpha1           | whether the replicas is 0                          |\\n\\n## New Features of CloneSet\\n\\n### Deletion Priority\\n\\nThe `controller.kubernetes.io/pod-deletion-cost` annotation was added to Kubernetes after version 1.21. `ReplicaSet` will sort the Kubernetes resources according to this cost value during scale in. CloneSet has supported the same feature since OpenKruise v0.9.0.\\n\\nUsers can configure this annotation in the pod. The int type of its value indicates the deletion cost of a certain pod compared to other pods under the same CloneSet. Pods with a lower cost have a higher deletion priority. If this annotation is not set, the deletion cost of the pod is 0 by default.\\n\\n*Note*: This deletion order is not determined solely by deletion cost. The real order serves like this:\\n\\n1. Not scheduled < scheduled\\n2. PodPending < PodUnknown < PodRunning\\n3. Not ready < ready\\n4. **Smaller pod-deletion cost < larger pod-deletion cost**\\n5. Period in the Ready state: short < long\\n6. Containers restart: more times < fewer times\\n7. Creation time: short < long\\n\\n### Image Pre-Download for In-Place Update\\n\\nWhen CloneSet is used for the in-place update of an application, only the container image is updated, while the Pod is not rebuilt. This ensures that the node where the Pod is located will not change. Therefore, if the CloneSet pulls the new image from all the Pod nodes in advance, the Pod in-place update speed will be improved substantially in subsequent batch releases.\\n\\nIf you want to use this feature in the current version, the feature-gate of `PreDownloadImageForInPlaceUpdate` needs to be explicitly enabled when installing or upgrading OpenKruise. If you update the images in the CloneSet template and the publish policy supports in-place update, CloneSet will create an `ImagePullJob` object automatically (the batch image pre-download function provided by OpenKruise) to download new images in advance on the node where the Pod is located.\\n\\nBy default, CloneSet sets the parallelism to 1 for `ImagePullJob`, which means images are pulled for one node and then another. For any adjustment, you can set the parallelism in the CloneSet annotation by executing the following code:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: CloneSet\\nmetadata:\\n  annotations:\\n    apps.kruise.io/image-predownload-parallelism: \\"5\\"\\n```\\n\\n### Pod Replacement by Scale Out and Scale In\\n\\nIn previous versions, the `maxUnavailable` and `maxSurge` policies of CloneSet only take effect during the application release process. In OpenKruise v0.9.0 and later versions, these two policies also function when deleting a specified Pod.\\n\\nWhen the user specifies one or more Pods to be deleted through `podsToDelete` or `apps.kruise.io/specified-delete`: true, CloneSet will only execute deletion when the number of unavailable Pods (of the total replicas) is less than the value of `maxUnavailable`. In addition, if the user has configured the `maxSurge` policy, the CloneSet will possibly create a new Pod first, wait for the new Pod to be ready, and then delete the old specified Pod.\\n\\nThe replacement method depends on the value of maxUnavailable and the number of unavailable Pods. For example:\\n\\n- For a CloneSet, `maxUnavailable=2, maxSurge=1` and only `pod-a` is unavailable. If you specify `pod-b` to be deleted, CloneSet will delete it promptly and create a new Pod.\\n- For a CloneSet, `maxUnavailable=1, maxSurge=1` and only `pod-a` is unavailable. If you specify `pod-b` to be deleted, CloneSet will create a new Pod, wait for it to be ready, and then delete the pod-b.\\n- For a CloneSet, `maxUnavailable=1, maxSurge=1` and only `pod-a` is unavailable. If you specify this `pod-a` to be deleted, CloneSet will delete it promptly and create a new Pod.\\n\\n### Efficient Rollback Based on Partition Final State\\n\\nIn the native workload, Deployment does not support phased release, while StatefulSet provides partition semantics to allow users to control the times of gray scale upgrades. OpenKruise workloads, such as CloneSet and Advanced StatefulSet, also provide partitions to support phased release.\\n\\nFor CloneSet, the semantics of Partition is **the number or percentage of Pods remaining in the old version**. For example, for a CloneSet with 100 replicas, if the partition value is changed in the sequence of 80 :arrow_right: 60 :arrow_right: 40 :arrow_right: 20 :arrow_right: 0 by steps during the image upgrade, the CloneSet is released in five batches.\\n\\nHowever, in the past, whether it is Deployment, StatefulSet, or CloneSet, if rollback is required during the release process, the template information (image) must be changed back to the old version. During the phased release of StatefulSet and CloneSet, reducing partition value will trigger the upgrade to a new version. Increasing partition value will not trigger rollback to the old version.\\n\\nThe partition of CloneSet supports the \\"final state rollback\\" function after v0.9.0. If the feature-gate `CloneSetPartitionRollback` is enabled when installing or upgrading OpenKruise, increasing the partition value will trigger CloneSet to roll back the corresponding number of new Pods to the old version.\\n\\nThere is a clear advantage here. During the phased release, only the partition value needs to be adjusted to flexibly control the numbers of old and new versions. However, the \\"old and new versions\\" for CloneSet correspond to `updateRevision` and `currentRevision` in its status:\\n\\n- updateRevision: The version of the template defined by the current CloneSet.\\n- currentRevision: The template version of CloneSet during the **previous successful full release**.\\n\\n### Short Hash\\n\\nBy default, the value of `controller-revision-hash` in Pod label set by CloneSet is the full name of the `ControllerRevision`. For example:\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  labels:\\n    controller-revision-hash: demo-cloneset-956df7994\\n```\\n\\nThe name is concatenated with the CloneSet name and the `ControllerRevision` hash value. Generally, the hash value is 8 to 10 characters in length. In Kubernetes, a label cannot exceed 63 characters in length. Therefore, the name of CloneSet cannot exceed 52 characters in length, or the Pod cannot be created.\\n\\nIn v0.9.0, the new feature-gate `CloneSetShortHash` is introduced. If it is enabled, CloneSet will set the value of `controller-revision-hash` in the Pod to a hash value only, like 956df7994. Therefore, the length restriction of the CloneSet name is eliminated. (CloneSet can still recognize and manage the Pod with revision labels in the full format, even if this function is enabled.)\\n\\n## New Features of SidecarSet\\n\\n### Sidecar Hot Upgrade Function\\n\\nSidecarSet is a workload provided by OpenKruise to manage sidecar containers separately. Users can inject and upgrade specified sidecar containers within a certain range of Pods using `SidecarSet`.\\n\\nBy default, for the independent in-place sidecar upgrade, the sidecar stops the container of the old version first and then creates a container of the new version. This method applies to sidecar containers that do not affect the Pod service availability, such as the log collection agent. However, for sidecar containers acting as a proxy such as Istio Envoy, this upgrade method is defective. Envoy, as a proxy container in the Pod, handles all the traffic. If users restart and upgrade directly, service availability will be affected. Thus, you need a complex grace termination and coordination mechanism to upgrade the envoy sidecar separately. Therefore, we offer a new solution for the upgrade of this kind of sidecar containers, namely, hot upgrade:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nspec:\\n  # ...\\n  containers:\\n  - name: nginx-sidecar\\n    image: nginx:1.18\\n    lifecycle:\\n      postStart:\\n        exec:\\n          command:\\n          - /bin/bash\\n          - -c\\n          - /usr/local/bin/nginx-agent migrate\\n    upgradeStrategy:\\n      upgradeType: HotUpgrade\\n      hotUpgradeEmptyImage: empty:1.0.0\\n```\\n\\n- `upgradeType`: `HotUpgrade` indicates that the type of the sidecar container is a hot upgrade, so the hot upgrade solution, `hotUpgradeEmptyImage`, will be executed. When performing a hot upgrade on the sidecar container, an empty container is required to switch services during the upgrade. The empty container has almost the same configuration as the sidecar container, except the image address, for example, command, lifecycle, and probe, but it does no actual work.\\n- `lifecycle.postStart`: State migration. This procedure completes the state migration during the hot upgrade. The script needs to be executed according to business characteristics. For example, NGINX hot upgrade requires shared Listen FD and traffic reloading.\\n\\n\\n## More\\n\\nFor more changes, please refer to the [release page](https://github.com/openkruise/kruise/releases) or [ChangeLog](https://github.com/openkruise/kruise/blob/master/CHANGELOG.md)."},{"id":"sidecarset","metadata":{"permalink":"/blog/sidecarset","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2021-03-15-sidecarSet-0.8.0.md","source":"@site/blog/2021-03-15-sidecarSet-0.8.0.md","title":"OpenKruise 0.8.0, A Powerful Tool for Sidecar Container Management","description":"OpenKruise is an open source management suite developed by Alibaba Cloud for cloud native application automation. It is currently a Sandbox project hosted under the Cloud Native Computing Foundation (CNCF). Based on years of Alibaba\'s experience in container and cloud native technologies, OpenKruise is a Kubernetes-based standard extension component that has been widely used in the Alibaba internal production environment, together with technical concepts and best practices for large-scale Internet scenarios.","date":"2021-03-15T00:00:00.000Z","formattedDate":"March 15, 2021","tags":[{"label":"workload","permalink":"/blog/tags/workload"},{"label":"sidecar","permalink":"/blog/tags/sidecar"}],"readingTime":10.885,"truncated":false,"authors":[{"name":"Mingshan Zhao","title":"Member of OpenKruise","url":"https://github.com/zmberg","imageURL":"https://github.com/zmberg.png","key":"zmberg"}],"frontMatter":{"slug":"sidecarset","title":"OpenKruise 0.8.0, A Powerful Tool for Sidecar Container Management","authors":["zmberg"],"tags":["workload","sidecar"]},"prevItem":{"title":"OpenKruise 0.9.0, Supports Pod Restart and Deletion Protection","permalink":"/blog/openkruise-0.9.0"},"nextItem":{"title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","permalink":"/blog/uniteddeployment"}},"content":"OpenKruise is an open source management suite developed by Alibaba Cloud for cloud native application automation. It is currently a Sandbox project hosted under the Cloud Native Computing Foundation (CNCF). Based on years of Alibaba\'s experience in container and cloud native technologies, OpenKruise is a Kubernetes-based standard extension component that has been widely used in the Alibaba internal production environment, together with technical concepts and best practices for large-scale Internet scenarios.\\n\\nOpenKruise released v0.8.0 on March 4, 2021, with enhanced SidecarSet capabilities, especially for log management of Sidecar.\\n\\n# Background\\nSidecar is a very important cloud native container design mode. It can create an independent Sidecar container by separating the auxiliary capabilities from the main container. In microservice architectures, the Sidecar mode is also used to separate general capabilities such as configuration management, service discovery, routing, and circuit breaking from main programs, thus making the microservice architectures less complicated. Since the popularity of Service Mesh has led to the prevalence of the Sidecar mode, the Sidecar mode has also been widely used within Alibaba Group to implement common capabilities such as O&M, security, and message-oriented middleware.\\n\\nIn Kubernetes clusters, pods can not only support the construction of main containers and Sidecar containers, but also many powerful workloads, such as deployment and statefulset to manage and upgrade the main containers and Sidecar containers. However, with the ever-growing businesses in Kubernetes clusters day by day, there have also been various Sidecar containers with a larger scale. Therefore, management and upgrades of online Sidecar containers are more complex:\\n\\n1. A business pod contains multiple Sidecar containers, such as O&M, security, and proxy containers. The business team should not only configure the main containers, but also learn to configure these Sidecar containers. This increases the workloads of the business team and the risks in Sidecar container configuration.\\n2. The Sidecar container needs to be restarted together with the main business container after the upgrade. The Sidecar container supports hundreds of online businesses, so it is extremely difficult to coordinate and promote the upgrades of a large number of online Sidecar containers.\\n3. If there are no effective updates for Sidecar containers with different online configurations and versions, it will pose great potential risks to the management of Sidecar containers.\\n\\nAlibaba Group has millions of containers with thousands of businesses. Therefore, the management and upgrades of Sidecar containers have become a major target for improvement. To this end, many internal requirements for the Sidecar containers have been summarized and integrated into OpenKruise. Finally, these requirements were abstracted as SidecarSet, a powerful tool to manage and upgrade a wide range of Sidecar containers.\\n\\n# OpenKruise SidecarSet\\nSidecarSet is an abstracted concept for Sidecar from OpenKruise. As one of the core workloads of OpenKruise, it is used to inject and upgrade the Sidecar containers in Kubernetes clusters. SidecarSet provides a variety of features so that users can easily manage Sidecar containers. The main features are as follows:\\n\\n1. **Separate configuration management:** Each Sidecar container is configured with separate SidecarSet configuration to facilitate management.\\n2. **Automatic injection:** Automatic Sidecar container injection is implemented in scenarios of Pod creation, Pod scale-out and Pod reconstruction.\\n3. **In-place upgrade:** Sidecar containers can be upgraded in-place without the reconstruction of any pods, so that the main business container is not affected. In addition, a wide range of gray release policies are included.\\n\\n**Note:** For a Pod that contains multiple container modes, the container that provides the main business logic to the external is the main container. Other containers provide auxiliary capabilities such as log collection, security, and proxy are Sidecar containers. For example, if a pod provides web capabilities outward, the nginx container that provides major web server capabilities is the main container. The logtail container is the Sidecar container that is responsible for collecting and reporting nginx logs. The SidecarSet resource abstraction in this article also solves some problems of the Sidecar containers.\\n\\n## Sidecar logging architectures\\nApplication logs allow you to see the internal running status of your application. Logs are useful for debugging problems and monitoring cluster activities. After the application is containerized, the simplest and most widely used logging is to write standard output and errors.\\n\\nHowever, in the current distributed systems and large-scale clusters, the above solution is not enough to meet the production environment standards. First, for distributed systems, logs are scattered in every single container, without a unified place for congregation. Logs may be lost in scenarios such as container crashes and Pod eviction. Therefore, there is a need for a more reliable log solution that is independent of the container lifecycle.\\n\\nSidecar logging architectures places the logging agent in an independent Sidecar container to collect container logs by sharing the log directory. Then, the logs are stored in the back-end storage of the log platform.\\n![logsidecar](../static/img/blog/2021-03-15-sidecarset/logsidecar.png)\\n\\nThis architecture is also used by Alibaba and Ant Group to realize the log collection of containers. Next, this article will explain how OpenKruise SidecarSet helps a large-scale implementation of the Sidecar log architecture in Kubernetes clusters.\\n\\n## Automatic Injection\\nOpenKruise SidecarSet has implemented automatic Sidecar container injection based on Kubernetes AdmissionWebhook mechanism. Therefore, as long as the Sidecar is configured in SidecarSet, the defined Sidecar container will be injected into the scaled pods with any deployment patterns, such as CloneSet, Deployment, or StatefulSet.\\n![inject sidecar](../static/img/blog/2021-03-15-sidecarset/inject_sidecar.png)\\n\\nThe owner of Sidecar containers only needs to configure SidecarSet to inject the Sidecar containers without affecting the business. This greatly reduces the threshold for using the Sidecar containers, and facilitates the management of Sidecar owners. In addition to containers, SidecarSet also extends the following fields to meet various scenarios of Sidecar injection:\\n```\\n# sidecarset.yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: test-sidecarset\\nspec:\\n  # Select Pods through the selector\\n  selector:\\n    matchLabels:\\n      app: web-server\\n  # Specify a namespace to take effect\\n  namespace: ns-1\\n  # container definition\\n  containers:\\n  - name: logtail\\n    image: logtail:1.0.0\\n    # Share the specified volume\\n    volumeMounts:\\n    - name: web-log\\n      mountPath: /var/log/web\\n    # Share all volumes\\n    shareVolumePolicy: disabled\\n    # Share environment variables\\n    transferEnv:\\n    - sourceContainerName: web-server\\n      envName: TZ\\n    volumes:\\n    - name: web-log\\n      emptyDir: {}\\n```\\n\\n### Pod selector\\n- **The selector is supported to select the pods to be injected.** In the above example, the pod of labels[app] = web-server is selected to inject the logtail container. Alternatively, labels[inject/logtail] = true can be added in all pods to inject a global Sidecar.\\n- **namespace:** SidecarSet is globally valid by default. This parameter can be configured to make it valid to a specific namespace.\\n\\n### Data volume sharing\\n- **Share the specified volume:** Use volumeMounts and volumes to share a specified volume with the main container. In the above example, a web-log volume is shared to achieve log collection.\\n- **Share all volumes:** Use shareVolumePolicy = enabled | disabled to specify whether to mount all volumes in the Pod\'s main container, which is often used for Sidecar containers such as log collection. If the configuration is enabled, all mount points in the application container are injected into the same Sidecar path, unless there are data volumes and mount points declared by Sidecar.\uff09\\n\\n### Share environment variables\\nUse transferEnv to obtain environment variables from other containers, which copies the environment variable named envName in the sourceContainerName container to the current Sidecar container. In the above example, the Sidecar container of logs shares the time zone of the main container, which is especially common in overseas environments.\\n\\n**Note:** The number of containers for the created Pods cannot be changed in the Kubernetes community. Therefore, the injection capability described above can only occur during the Pod creation phase. For the created Pods, Pod reconstruction is required for injection.\\n\\n## In-place Upgrade\\nSidecarSet not only allows to inject the Sidecar containers, but also reuses the in-place update feature of OpenKruise. This realizes the upgrade of the Sidecar containers without restarting the Pod and the main container. Since this upgrade method does not affect the business, upgrading the Sidecar containers is no longer a pain point. Thus, it brings a lot of conveniences to Sidecar owners and speeds up the Sidecar version iteration.\\n\\n![inplace sidecar](../static/img/blog/2021-03-15-sidecarset/inplace_sidecar.png)\\n\\n**Note:** Only the modification on the container.image fields for the created Pods is allowed by the Kubernetes community. Therefore, the modification on other fields of Sidecar containers requires the reconstruction of Pod, and the in-place upgrade is not supported.\\n\\nTo meet the requirements in some complex Sidecar upgrade scenarios, SidecarSet provides the in-place upgrade and a wide range of gray release strategies.\\n\\n## Gray Release\\nGray release is a common method that allows a Sidecar container to be released smoothly. It is highly recommended that this method is used in large-scale clusters. Here is an example of Pod\'s rolling release based on the maximum unavailability after the first batch of Pod release is suspended. Suppose that there are 1,000 Pods to be released:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\nspec:\\n  # ...\\n  updateStrategy:\\n    type: RollingUpdate\\n    partition: 980\\n    maxUnavailable: 10%\\n```\\n\\nThe configuration above is suspended after the former release of 20 pods (1000 \u2013 980 = 20). After the Sidecar container has been normal for a period of time in business, adjust the update SidecarSet configuration:\\n\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\nspec:\\n  # ...\\n  updateStrategy:\\n    type: RollingUpdate\\n    maxUnavailable: 10%\\n```\\n\\nAs such, the remaining 980 Pods will be released in the order of the maximum unavailable numbers (10% * 1000=100) until all Pods are released.\\n\\nPartition indicates that the number or percentage of Pods of the old version is retained, with the default value of 0. Here, the partition does not represent any order number. If the partition is set up during the release process:\\n- If it is a number, the controller will update the pods with (replicas \u2013 partition) to the latest version.\\n- If it is a percentage, the controller will update the pods with (replicas * (100% - partition)) to the latest version.\\n\\nMaxUnavailable indicates the maximum unavailable number of pods at the same time during the release, with the default value of 1. Users can set the MaxUnavailable value as absolute value or percentage. The percentage is used by the controller to calculate the absolute value based on the number of selected pods.\\n\\n**Note:** The values of maxUnavailable and partition are not necessarily associated. For example:\\n- Under {matched pod} = 100, partition = 50, and maxUnavailable = 10, the controller will release 50 pods to the new version, but the release is limited to 10. That is, only 10 pods are released at the same time. A pod is released one after another until 50 pods are all released.\\n- Under {matched pod} = 100, partition = 80, and maxUnavailable = 30, the controller will release 20 Pods to the new version. The controller releases all 20 pods at the same time because the number of maxUnavailable is met.\\n\\n## Canary Release\\nFor businesses that require canary release, strategy.selector can be considered as a choice. The solution is to mark the labels[canary.release] = true into the Pods that require canary release, and then use strategy.selector.matchLabels to select the pods.\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\nspec:\\n  # ...\\n  updateStrategy:\\n    type: RollingUpdate\\n    selector:\\n      matchLabels:\\n      - canary.release: true\\n    maxUnavailable: 10%\\n```\\n\\nThe above configuration only releases the containers marked with canary labels. After the canary verification is completed, rolling release is performed based on the maximum unavailability by removing the configuration of updateStrategy.selector.\\n\\n## Scatter Release\\nThe upgrade sequence of pods in SidecarSet is subject to the following rules by default:\\n- For the pod set upgrade, multiple upgrades with the same order are guaranteed.\\n- The selection priority is (the smaller, the more prioritized): unscheduled < scheduled, pending < unknown < running, not-ready < ready, newer pods < older pods.\\n\\nIn addition to the above default release order, the scatter release policy allows users to scatter the pods that match certain labels to the entire release process. For example, for a global sidecar container like logtail, dozens of business pods may be injected into a cluster. Thus, the logtail can be released after being scattered based on the application name, realizing scattered and gray release among applications. And it can be performed together with the maximum unavailability.\\n```yaml\\napiVersion: apps.kruise.io/v1alpha1\\nkind: SidecarSet\\nmetadata:\\n  name: sidecarset\\nspec:\\n# ...\\n  updateStrategy:\\n    type: RollingUpdate\\n    scatterStrategy:\\n    - key: app_name\\n      value: nginx\\n    - key: app_name\\n      value: web-server\\n    - key: app_name\\n      value: api-gateway\\n    maxUnavailable: 10%\\n```\\n\\nNote: In the current version, all application names must be listed. In the next version, an intelligent scattered release will be supported with only the label key configured.\\n\\n# Summary\\nIn the OpenKruise v0.8.0, the SidecarSet has been improved in terms of log management in Sidecar scenarios. In the later exploration of the stability and performance of SidecarSet, more scenarios will be covered at the same time. For example, Service Mesh scenario will be supported in the next version. Moreover, more people are welcomed to participate OpenKruise community to improve the application management and delivery extensibility of Kubernetes for scenarios featuring large scale, complexity and extreme performance."},{"id":"uniteddeployment","metadata":{"permalink":"/blog/uniteddeployment","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2019-11-20-uniteddeployment.md","source":"@site/blog/2019-11-20-uniteddeployment.md","title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","description":"Ironically, probably every cloud user knew (or should realized that) failures in Cloud resources","date":"2019-11-20T00:00:00.000Z","formattedDate":"November 20, 2019","tags":[{"label":"workload","permalink":"/blog/tags/workload"},{"label":"uniteddeployment","permalink":"/blog/tags/uniteddeployment"}],"readingTime":6.005,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"}],"frontMatter":{"slug":"uniteddeployment","title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","authors":["Fei-Guo"],"tags":["workload","uniteddeployment"]},"prevItem":{"title":"OpenKruise 0.8.0, A Powerful Tool for Sidecar Container Management","permalink":"/blog/sidecarset"},"nextItem":{"title":"Learning Concurrent Reconciling","permalink":"/blog/learning-concurrent-reconciling"}},"content":"Ironically, probably every cloud user knew (or should realized that) failures in Cloud resources\\nare inevitable. Hence, high availability is probably one of the most desirable features that\\nCloud Provider offers for cloud users. For example, in AWS, each geographic region has \\nmultiple isolated locations known as Availability Zones (AZs). \\nAWS provides various AZ-aware solutions to allow the compute or storage resources of the user\\napplications to be distributed across multiple AZs in order to tolerate AZ failure, which indeed\\nhappened in the past. \\n\\nIn Kubernetes, the concept of AZ is not realized by an API object. Instead,\\nan AZ is usually represented by a group of hosts that have the same location label.\\nAlthough hosts within the same AZ can be identified by labels, the capability of distributing Pods across\\nAZs was missing in Kubernetes default scheduler. Hence it was difficult to use single \\n`StatefulSet` or `Deployment` to perform  AZ-aware Pods deployment. Fortunately, \\nin Kubernetes 1.16, a new feature called [\\"Pod Topology Spread Constraints\\"](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/)\\nwas introduced. Users now can add new constraints in the Pod Spec, and scheduler\\nwill enforce the constraints so that Pods can be distributed across failure \\ndomains such as AZs, regions or nodes, in a uniform fashion.\\n\\nIn Kruise, **UnitedDeploymemt** provides an alternative to achieve high availability in\\na cluster that consists of multiple fault domains - that is, managing multiple homogeneous \\nworkloads, and each workload is dedicated to a single `Subset`. Pod distribution across AZs is\\ndetermined by the replica number of each workload.\\nSince each `Subset` is associated with a workload, UnitedDeployment can support\\nfiner-grained rollout and deployment strategies. \\nIn addition, UnitedDeploymemt can be further extended to support\\nmultiple clusters! Let us reveal how UnitedDeployment is designed.\\n\\n\\n## Using `Subsets` to describe domain topology\\n\\nUnitedDeploymemt uses `Subset` to represent a failure domain. `Subset` API\\nprimarily specifies the nodes that forms the domain and the number of replicas, or\\nthe percentage of total replicas, run in this domain. UnitedDeployment manages\\nsubset workloads against a specific domain topology, described by a `Subset` array.\\n\\n```\\ntype Topology struct {\\n\\t// Contains the details of each subset.\\n\\tSubsets []Subset\\n}\\n\\ntype Subset struct {\\n\\t// Indicates the name of this subset, which will be used to generate\\n\\t// subset workload name prefix in the format \'<deployment-name>-<subset-name>-\'.\\n\\tName string\\n\\n\\t// Indicates the node select strategy to form the subset.\\n\\tNodeSelector corev1.NodeSelector\\n\\n\\t// Indicates the number of the subset replicas or percentage of it on the\\n\\t// UnitedDeployment replicas.\\n\\tReplicas *intstr.IntOrString\\n}\\n```\\n\\nThe specification of the subset workload is saved in `Spec.Template`. UnitedDeployment\\nonly supports `StatefulSet` subset workload as of now. An interesting part of `Subset`\\ndesign is that now user can specify **customized Pod distribution** across AZs, which is not\\nnecessarily a uniform distribution in some cases. For example, if the AZ\\nutilization or capacity are not homogeneous, evenly distributing Pods may lead to Pod deployment\\nfailure due to lack of resources. If users have prior knowledge about AZ resource capacity/usage,\\nUnitedDeployment can help to apply an optimal Pod distribution to ensure overall\\ncluster utilization remains balanced. Of course, if not specified, a uniform Pod distribution\\nwill be applied to maximize availability.\\n\\n## Customized subset rollout `Partitions`\\n\\nUser can update all the UnitedDeployment subset workloads by providing a\\nnew version of subset workload template.\\nNote that UnitedDeployment does not control\\nthe entire rollout process of all subset workloads, which is typically done by another rollout\\ncontroller built on top of it. Since the replica number in each `Subset` can be different,\\nit will be much more convenient to allow user to specify the individual rollout `Partition` of each\\nsubset workload instead of using one `Partition` to rule all, so that they can be upgraded in the same pace.\\nUnitedDeployment provides `ManualUpdate` strategy to customize per subset rollout `Partition`.\\n\\n```\\ntype UnitedDeploymentUpdateStrategy struct {\\n\\t// Type of UnitedDeployment update.\\n\\tType UpdateStrategyType\\n\\t// Indicates the partition of each subset.\\n\\tManualUpdate *ManualUpdate\\n}\\n\\ntype ManualUpdate struct {\\n\\t// Indicates number of subset partition.\\n\\tPartitions map[string]int32\\n}\\n```\\n\\n![multi-cluster controller](/img/blog/2019-11-20-uniteddeployment/uniteddeployment-1.png)\\n\\nThis makes it fairly easy to coordinate multiple subsets rollout. For example,\\nas illustrated in Figure 1, assuming UnitedDeployment manages three subsets and\\ntheir replica numbers are 4, 2, 2 respectively, a rollout \\ncontroller can realize a canary release plan of upgrading 50% of Pods in each\\nsubset at a time by setting subset partitions to 2, 1, 1 respectively. \\nThe same cannot be easily achieved by using a single workload controller like `StatefulSet`\\nor `Deployment`.\\n\\n## Multi-Cluster application management (In future)\\n\\nUnitedDeployment can be extended to support multi-cluster workload\\nmanagement. The idea is that `Subsets` may not only\\nreside in one cluster, but also spread over multiple clusters. \\nMore specifically, domain topology specification will associate\\na `ClusterRegistryQuerySpec`, which describes the clusters that UnitedDeployment\\nmay distribute Pods to. Each cluster is represented by a custom resource managed by a\\nClusterRegistry controller using Kubernetes [cluster registry APIs](https://github.com/kubernetes/cluster-registry).\\n\\n```\\ntype Topology struct {\\n  // ClusterRegistryQuerySpec is used to find the all the clusters that\\n  // the workload may be deployed to. \\n  ClusterRegistry *ClusterRegistryQuerySpec\\n  // Contains the details of each subset including the target cluster name and\\n  // the node selector in target cluster.\\n  Subsets []Subset\\n}\\n\\ntype ClusterRegistryQuerySpec struct {\\n  // Namespaces that the cluster objects reside.\\n  // If not specified, default namespace is used.\\n  Namespaces []string\\n  // Selector is the label matcher to find all qualified clusters.\\n  Selector   map[string]string\\n  // Describe the kind and APIversion of the cluster object.\\n  ClusterType metav1.TypeMeta\\n}\\n\\ntype Subset struct {\\n  Name string\\n\\n  // The name of target cluster. The controller will validate that\\n  // the TargetCluster exits based on Topology.ClusterRegistry.\\n  TargetCluster *TargetCluster\\n\\n  // Indicate the node select strategy in the Subset.TargetCluster.\\n  // If Subset.TargetCluster is not set, node selector strategy refers to\\n  // current cluster.\\n  NodeSelector corev1.NodeSelector\\n\\n  Replicas *intstr.IntOrString \\n}\\n\\ntype TargetCluster struct {\\n  // Namespace of the target cluster CRD\\n  Namespace string\\n  // Target cluster name\\n  Name string\\n}\\n```\\n\\nA new `TargetCluster` field is added to the `Subset` API. If it presents, the\\n`NodeSelector` indicates the node selection logic in the target cluster. Now\\nUnitedDeployment controller can distribute application Pods to multiple clusters by\\ninstantiating a `StatefulSet` workload in each target cluster with a specific\\nreplica number (or a percentage of total replica), as illustrated in Figure 2.\\n\\n![multi-cluster\\tcontroller](/img/blog/2019-11-20-uniteddeployment/uniteddeployment-2.png)\\n\\nAt a first glance, UnitedDeployment looks more like a federation\\ncontroller following the design pattern of [Kubefed](https://github.com/kubernetes-sigs/kubefed),\\nbut it isn\'t. The fundamental difference is that Kubefed focuses on propagating arbitrary\\nobject types to remote clusters instead of managing an application across clusters. \\nIn this example, had a Kubefed style controller been used, each `StatefulSet` workload in\\nindividual cluster would have a replica of 100. UnitedDeployment focuses more on\\nproviding the ability of managing multiple workloads in multiple clusters on behalf\\nof one application, which is absent in Kubernetes community to the best of our\\nknowledge.\\n\\n## Summary\\n\\nThis blog post introduces UnitedDeployment, a new controller which helps managing \\napplication spread over multiple domains (in arbitrary clusters). \\nIt not only allows evenly distributing Pods over AZs, \\nwhich arguably can be more efficiently done using the new Pod Topology Spread\\nConstraint APIs though, but also enables flexible workload deployment/rollout and\\nsupports multi-cluster use cases in the future."},{"id":"learning-concurrent-reconciling","metadata":{"permalink":"/blog/learning-concurrent-reconciling","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2019-11-10-learning-concurrent-reconciling.md","source":"@site/blog/2019-11-10-learning-concurrent-reconciling.md","title":"Learning Concurrent Reconciling","description":"The concept of controller in Kubernete is one of the most important reasons that make it successful.","date":"2019-11-10T00:00:00.000Z","formattedDate":"November 10, 2019","tags":[{"label":"workload","permalink":"/blog/tags/workload"},{"label":"reconcile","permalink":"/blog/tags/reconcile"},{"label":"controller","permalink":"/blog/tags/controller"}],"readingTime":3.915,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"}],"frontMatter":{"slug":"learning-concurrent-reconciling","title":"Learning Concurrent Reconciling","authors":["Fei-Guo"],"tags":["workload","reconcile","controller"]},"prevItem":{"title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","permalink":"/blog/uniteddeployment"},"nextItem":{"title":"Kruise Workload Classification Guidance","permalink":"/blog/workload-classification-guidance"}},"content":"The concept of controller in Kubernete is one of the most important reasons that make it successful.\\nController is the core mechanism that supports Kubernetes APIs to ensure the system reaches \\nthe desired state. By leveraging CRDs/controllers and operators, it is fairly easy for \\nother systems to integrate with Kubernetes. \\n\\nController runtime library and the corresponding controller tool [KubeBuilder](https://book.kubebuilder.io/introduction.html)\\nare widely used by many developers to build their customized Kubernetes controllers. In Kruise project,\\nwe also use Kubebuilder to generate scaffolding codes that implement the \\"reconciling\\" logic. \\nIn this blog post, I will share some learnings from\\nKruise controller development, particularly, about concurrent reconciling. \\n\\nSome people may already notice that controller runtime supports concurrent reconciling.\\nCheck for the options ([source](https://github.com/kubernetes-sigs/controller-runtime/blob/81842d0e78f7111f0566156189806e2801e3adf1/pkg/controller/controller.go#L32))\\nused to create new controller:  \\n\\n```\\ntype Options struct {\\n\\t// MaxConcurrentReconciles is the maximum number of concurrent Reconciles which can be run. Defaults to 1.\\n\\tMaxConcurrentReconciles int\\n\\n\\t// Reconciler reconciles an object\\n\\tReconciler reconcile.Reconciler\\n}\\n```\\n\\nConcurrent reconciling is quite useful when the states of the controller\'s watched objects change so\\nfrequently that a large amount of reconcile requests are sent to and queued in the reconcile queue.\\nMultiple reconcile loops do help drain the reconcile queue much more quickly compared to the default single\\nreconcile loop case. Although this is a great feature for performance, without digging into the code,\\nan immediate concern that a developer may raise is that will this introduce consistency issue? \\ni.e., is it possible that two reconcile loops handle the same object at the same time?\\n\\nThe answer is NO, as you may expect. The \\"magic\\" is enforced by the workqueue\\nimplementation in Kubernetes `client-go`, which is used by controller runtime reconcile queue. \\nThe workqueue algorithm ([source](https://github.com/kubernetes/client-go/blob/a57d0056dbf1d48baaf3cee876c123bea745591f/util/workqueue/queue.go#L65))\\nis demonstrated in Figure 1.\\n\\n![workqueue](/img/blog/2019-11-10-learning-concurrent-reconciling/workqueue.png)\\n\\nBasically, the workqueue uses a `queue` and two `sets` to coordinate the process of handling multiple reconciling \\nrequests against the same object. Figure 1(a) presents the initial state of handling four reconcile requests,\\ntwo of which target the same object A. When a request arrives, the target object is first added to the `dirty set`\\nor dropped if it presents in `dirty set`,  and then pushed to the `queue` only if it is not presented in\\n`processing set`. Figure 1(b) shows the case of adding three requests consecutively. \\nWhen a reconciling loop is ready to serve a request, it gets the target object from the `front` of the queue. The\\nobject is also added to the `processing set` and removed from the `dirty set` (Figure 1(c)).\\nNow if a request of the processing object arrives, the object is only added to the `dirty set`, not\\nto the `queue` (Figure 1(d)). This guarantees that an object is only handled by one reconciling\\nloop. When reconciling is done, the object is removed from the `processing set`. If the object is also\\nshown in the `dirty set`, it is added back to the `back` of the `queue` (Figure 1(e)).\\n\\nThe above algorithm has following implications:\\n* It avoids concurrent reconciling for the same object.\\n* The object processing order can be different from arriving order even if there is only one reconciling thread.\\nThis usually would not be a problem since the controller still reconciles to the final cluster state. However,\\nthe out of order reconciling may cause a significant delay for a request. \\n![workqueue-starve](/img/blog/2019-11-10-learning-concurrent-reconciling/workqueue-starve.png).... For example, as illustrated in \\nFigure 2, assuming there is only one reconciling thread and two requests targeting the same object A arrive, one of\\nthem will be processed and object A will be added to the `dirty set` (Figure 2(b)). \\nIf the reconciling takes a long time and during which a large number of new reconciling requests arrive,\\nthe queue will be filled up by the new requests (Figure 2(c)). When reconciling is done, object A will be\\nadded to the `back` of the `queue` (Figure 2(d)). It would not be handled until all the requests coming after had been\\nhandled, which can cause a noticeable long delay. The workaround is actually simple - **USE CONCURRENT RECONCILES**.\\nSince the cost of an idle go routine is fairly small, the overhead of having multiple reconcile threads is\\nlow even if the controller is idle. It seems that the `MaxConcurrentReconciles` value should\\nbe overwritten to a value larger than the default 1 (CloneSet uses 10 for example).\\n* Last but not the least, reconcile requests can be dropped (if the target exists in `dirty set`). This means\\nthat we cannot assume that the controller can track all the object state change events. Recalling a presentation\\ngiven by [Tim Hockin](https://speakerdeck.com/thockin/edge-vs-level-triggered-logic), Kubernetes controller\\nis level triggered, not edge triggered. It reconciles for state, not for events. \\n\\nThanks for reading the post, hope it helps."},{"id":"workload-classification-guidance","metadata":{"permalink":"/blog/workload-classification-guidance","editUrl":"https://github.com/openkruise/openkruise.io/edit/master/blog/2019-10-10-workload-classification-guidance.md","source":"@site/blog/2019-10-10-workload-classification-guidance.md","title":"Kruise Workload Classification Guidance","description":"Kubernetes does not provide a clear guidance about which controller is the best fit for","date":"2019-10-10T00:00:00.000Z","formattedDate":"October 10, 2019","tags":[{"label":"workload","permalink":"/blog/tags/workload"}],"readingTime":5.11,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"},{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"frontMatter":{"slug":"workload-classification-guidance","title":"Kruise Workload Classification Guidance","authors":["Fei-Guo","FillZpp"],"tags":["workload"]},"prevItem":{"title":"Learning Concurrent Reconciling","permalink":"/blog/learning-concurrent-reconciling"}},"content":"Kubernetes does not provide a clear guidance about which controller is the best fit for\\na user application. Sometimes, this does not seem to be a big problem if users understand\\nboth the application and workload well. For example, users usually know when to choose\\n`Job/CronJob` or `DaemonSet` since the concepts of these workload are straightforward -\\nthe former is designed for temporal batch style applications and the latter is suitable\\nfor long running Pod which is distributed in every node. On the other hand, the usage\\nboundary between `Deployment` and `StatefulSet` is vague. An application managed by\\na `Deployment` conceptually can be managed by a `StatefulSet` as well, the opposite may\\nalso apply as long as the Pod `OrderedReady` capability of `StatefulSet` is not mandatory.\\nFurthermore, as more and more customized controllers/operators become available in Kubernetes\\ncommunity, finding suitable controller can be a nonnegligible user problem especially\\nwhen some controllers have functional overlaps.\\n\\nKruise attempts to mitigate the problem from two aspects:\\n* Carefully design the new controllers in the Kruise suite to avoid unnecessary functional\\nduplications that may confuse users.\\n* Establish a classification mechanism for existing workload controllers so that user\\ncan more easily understand the use cases of them. We will elaborate this more in this\\npost. The first and most intuitive criterion for classification is the controller name.\\n\\n### Controller Name Convention\\nAn easily understandable controller name can certainly help adoption. After consulting\\nwith many internal/external Kubernetes users, we decide to use the following naming\\nconventions in Kruise. Note that these conventions are not contradicted with the controller\\nnames used in upstream controllers.\\n\\n* **Set** -suffix names: This type of controller manages Pods directly. Examples\\ninclude `CloneSet`, `ReplicaSet` and `SidecarSet`. It supports\\nvarious depolyment/rollout strategies in Pod level.\\n\\n* **Deployment** -suffix names: This type of controller does not manage Pods\\ndirectly. Instead, it manages one or many **Set** -suffix workload instances which are\\ncreated on behalf of one application. The controller can provide capabilities\\nto orchestrate the deployment/rollout of multiple instances. For example, `Deployment`\\nmanages `ReplicaSet` and provides rollout capability which is not available in `ReplicaSet`.\\n`UnitedDeployment` (planned in [M3 release](https://github.com/openkruise/kruise/projects))\\nmanages multiple `StatefulSet` created in respect of multiple domains\\n(i.e., fault domains) within one cluster.\\n\\n* **Job** -suffix names: This type of controller manages batch style applications with\\ndifferent depolyment/rollout strategies. For example, `BroadcastJob` distributes a\\njob style Pod to every node in the cluster.\\n\\n**Set**, **Deployment** and **Job** are widely adopted terms in Kubernetes community.\\nKruise leverages them with certain extensions.\\n\\nCan we further distinguish controllers with the same name suffix? Normally the string prior to\\nthe suffix should be self-explainable, but in many cases it is hard to find a right word to\\ndescribe what the controller does. Check to see how `StatefulSet` is originated in\\nthis [thread](https://github.com/kubernetes/kubernetes/issues/27430). It takes four\\nmonths for community to decide to use the name `StatefulSet` to replace the original\\nname `PetSet` although the new name still confuse people by looking\\nat its API documentation. This example showcases that sometimes a well-thought-out name\\nmay not be helpful to identify controller. Again, Kruise does not plan to resolve\\nthis problem. As an incremental effort, Kruise considers the following criterion to help classify\\n**Set** -suffix controllers.\\n\\n\\n### Fixed Pod Name\\nOne unique property of `StatefulSet` is that it maintains consistent identities for\\nPod network and storage. Essentially, this is done by fixing Pod names.\\nPod name can identify both network and storage since it is part of DNS record and\\ncan be used to name Pod volume claim. Why is this property needed given that all Pods in\\n`StatefulSet` are created from the same Pod template?\\nA well known use case is to manage distributed coordination server application such as\\netcd or Zookeeper. This type of application requires the cluster member\\n(i.e., the Pod) to access the same data (in Pod volume) whenever a member is\\nreconstructed upon failure, in order to function correctly. To differentiate the term\\n`State` in `StatefulSet` from the same term used in other computer science areas,\\nI\'d like to associate `State` with Pod name in this document. That being said, controllers\\nlike `ReplicaSet` and `DaemonSet` are `Stateless` since they don\'t require to reuse the\\nold Pod name when a Pod is recreated.\\n\\nSupporting `Stateful` does lead to inflexibility for controller. `StatefulSet` relies on ordinal\\nnumbers to realize fixing Pod names. The workload rollout and scaling\\nhas to be done in a strict order. As a consequence, some useful enhancements to `StatefulSet`\\nbecome impossible. For example,\\n* Selective Pod upgrade and Pod deletion (when scale in). These features can be helpful\\nwhen Pods are spread across different regions or fault domains.\\n* The ability of taking control over existing Pods with arbitrary names. There are\\ncases where Pod creation is done by one controller but Pod lifecycle management\\nis done by another controller (e.g., `StatefulSet`).\\n\\nWe found that many containerized applications do not require the `Stateful` property\\nof fixing Pod names, and `StatefulSet` is hard to be extended for those\\napplications in many cases. To fill the gap, Kruise has released a new controller\\ncalled `CloneSet` to manage the `Stateless` applications. In a nutshell, `CloneSet`\\nprovides PVC support and enriched rollout and management capabilities.\\nThe following table roughly compares Advanced StatefulSet and CloneSet in a few aspects.\\n\\n| Features   |     Advanced StatefulSet      |  CloneSet |\\n|----------|:-------------:|:------:|\\n| PVC | Yes | Yes |\\n| Pod name | Ordered | Random |\\n| Inplace upgrade | Yes | Yes |\\n| Max unavailable | Yes | Yes |\\n| Selective deletion | No | Yes |\\n| Selective upgrade | No | Yes |\\n| Change Pod ownership | No | Yes |\\n\\nNow, a clear recommendation to Kruise users is if your applications require fixed Pod names (identities for Pod network and storage), you can start with `Advanced StatefulSet`.\\nOtherwise, `CloneSet` is the primary choice of **Set** -suffix controllers (if `DaemonSet` is not\\napplicable).\\n\\n### Summary\\nKruise aims to provide intuitive names for new controllers. As a supplement, this post\\nprovides additional guidance for Kruise users to pick the right controller for their\\napplications. Hope it helps!"}]}')}}]);